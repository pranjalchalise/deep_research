{
  "metadata": {
    "key": "search:general:6:notable researchers and contributors to retrieval augmented generation",
    "created": 1769979726.137152,
    "ttl": 86400
  },
  "data": [
    {
      "url": "https://medium.com/@custom_aistudio/this-history-of-retrieval-augmented-generation-in-3-minutes-f7f07073599a",
      "title": "This history of Retrieval-Augmented Generation in 3 ...",
      "snippet": "The roots of RAG can be traced back to the 1950s and 1960s when information retrieval systems were being used. Researchers like Hans Peter Luhn and Gerald Salton laid the groundwork with vector space models and term frequency-inverse document frequency (TF-IDF), enabling computers to retrieve documents relevant to user queries. This was a strong first leap towards RAG! [...] ## The Emergence of RAG\n\nThe formalization of Retrieval-Augmented Generation as a distinct paradigm came with the landmark paper “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks” by Patrick Lewis and colleagues in 2020. The authors proposed a system where an external retriever fetches relevant documents in response to a query, and a generative model conditioned on this retrieved knowledge produces a response. This approach addressed key challenges faced by large language models (LLMs), such as hallucinations and outdated knowledge, by grounding their output in external, up-to-date information. [...] Sitemap\n\nOpen in app\n\nSign in\n\nWrite\n\nSearch\n\nSign in\n\n# This history of Retrieval-Augmented Generation in 3 minutes…! Updated August 3, 2025\n\nRoss W. Green, MD (CustomAI Studio)\n\n5 min read\n\n·\n\nJan 27, 2025\n\n--\n\nThe concept of Retrieval-Augmented Generation (RAG) is a culmination of decades of research in two distinct fields: information retrieval (IR) and natural language generation (NLG). Its history reflects a gradual integration of these fields, driven by the need to create systems capable of providing accurate, contextual, and dynamic responses in knowledge-intensive scenarios.\n\n## Early Foundations: Retrieval and Generation as Separate Disciplines"
    },
    {
      "url": "https://arxiv.org/abs/2507.18910",
      "title": "A Systematic Review of Key Retrieval-Augmented Generation (RAG ...",
      "snippet": "[2507.18910] A Systematic Review of Key Retrieval-Augmented Generation (RAG) Systems: Progress, Gaps, and Future Directions\n\nSkip to main content\n\nImage 1: Cornell University Logo\n\nWe gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.Donate\n\n \n\n [Submitted on 25 Jul 2025]\n\nTitle:A Systematic Review of Key Retrieval-Augmented Generation (RAG) Systems: Progress, Gaps, and Future Directions\n\nAuthors:Agada Joseph Oche, Ademola Glory Folashade, Tirthankar Ghosal, Arpan Biswas\n\nView a PDF of the paper titled A Systematic Review of Key Retrieval-Augmented Generation (RAG) Systems: Progress, Gaps, and Future Directions, by Agada Joseph Oche and 3 other authors [...] Comments:33 pages, 2 figures\nSubjects:Computation and Language (cs.CL); Machine Learning (cs.LG)\nCite as:arXiv:2507.18910 [cs.CL]\n(or arXiv:2507.18910v1 [cs.CL] for this version)\n\nFocus to learn more\n\n arXiv-issued DOI via DataCite\n\nSubmission history\n\n From: Arpan Biswas [view email] \n\n[v1] Fri, 25 Jul 2025 03:05:46 UTC (229 KB)\n\n Systems: Progress, Gaps, and Future Directions, by Agada Joseph Oche and 3 other authors\n\n   View PDF\n   HTML (experimental)\n   TeX Source\n\nImage 5: license iconview license\n\n Current browse context: \n\ncs.CL\n\n<prev\") | next>\")\n\nnew | recent | 2025-07\n\n Change to browse by: \n\ncs\n\ncs.LG\n\n### References & Citations\n\n   NASA ADS\n   Google Scholar\n   Semantic Scholar\n\nexport BibTeX citation Loading...\n\nBibTeX formatted citation\n\n× [...] > Abstract:Retrieval-Augmented Generation (RAG) represents a major advancement in natural language processing (NLP), combining large language models (LLMs) with information retrieval systems to enhance factual grounding, accuracy, and contextual relevance. This paper presents a comprehensive systematic review of RAG, tracing its evolution from early developments in open domain question answering to recent state-of-the-art implementations across diverse applications. The review begins by outlining the motivations behind RAG, particularly its ability to mitigate hallucinations and outdated knowledge in parametric models. Core technical components-retrieval mechanisms, sequence-to-sequence generation models, and fusion strategies are examined in detail. A year-by-year analysis highlights key"
    },
    {
      "url": "https://arxiv.org/abs/2410.12837",
      "title": "[2410.12837] A Comprehensive Survey of Retrieval ...",
      "snippet": "[2410.12837] A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions\n\nSkip to main content\n\nImage 1: Cornell University Logo\n\nWe gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.Donate\n\n \n\n [Submitted on 3 Oct 2024]\n\nTitle:A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions\n\nAuthors:Shailja Gupta, Rajesh Ranjan, Surya Narayan Singh\n\nView a PDF of the paper titled A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions, by Shailja Gupta and 2 other authors [...] > Abstract:This paper presents a comprehensive study of Retrieval-Augmented Generation (RAG), tracing its evolution from foundational concepts to the current state of the art. RAG combines retrieval mechanisms with generative language models to enhance the accuracy of outputs, addressing key limitations of LLMs. The study explores the basic architecture of RAG, focusing on how retrieval and generation are integrated to handle knowledge-intensive tasks. A detailed review of the significant technological advancements in RAG is provided, including key innovations in retrieval-augmented language models and applications across various domains such as question-answering, summarization, and knowledge-based tasks. Recent research breakthroughs are discussed, highlighting novel methods for"
    },
    {
      "url": "https://drpress.org/ojs/index.php/HSET/article/view/28756",
      "title": "A Research of Challenges and Solutions in Retrieval ...",
      "snippet": "## Downloads\n\nDownload data is not yet available.\n\n## References\n\n Lewis, Patrick, Scott Reed, Jack Urbanek, and Nando de Freitas. Retrieval-augmented generation for knowledge-intensive NLP tasks. Advances in Neural Information Processing Systems 33 2020: 9459-9474.\n\n Borgeaud, Sebastian, Arthur Mensch, Guillaume Lample, and Marc'Aurelio Ranzato. Improving language models by retrieving from trillions of tokens. International conference on machine learning. PMLR, 2022.\n\n Izacard, Gautier, and Edouard Grave. Atlas: Few-shot learning with retrieval augmented language models. Journal of Machine Learning Research 24.251 2023: 1-43. [...] # A Research of Challenges and Solutions in Retrieval Augmented Generation (RAG) Systems\n\n## Authors\n\n Jiafeng Gu\n\n## DOI:\n\n \n\n## Keywords:\n\n Retrieval augmented generation, natural language processing, information retrieval, knowledge base.\n\n## Abstract [...] Peng, Dehua, Zhipeng Gui, and Huayi Wu. Interpreting the curse of dimensionality from distance concentration and manifold effect. arXiv preprint arXiv:2401.00422 2023.\n\n Hassantabar, Shayan, Zeyu Wang, and Niraj K. Jha. SCANN: Synthesis of compact and accurate neural networks. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems 41.9 2021: 3012-3025.\n\n Dhingra, Bhuwan, and Graham Neubig. Time-aware language models as temporal knowledge bases. Transactions of the Association for Computational Linguistics 10 2022: 257-273.\n\n GAO, Yunfan, Zhenzhong LAN, and Jianfeng GAO. Retrieval-augmented generation for large language models: A survey. ArXiv preprint arXiv: 2312.10997 2023."
    },
    {
      "url": "https://arxiv.org/abs/2005.11401",
      "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
      "snippet": "Authors:Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela\n\nView a PDF of the paper titled Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, by Patrick Lewis and 11 other authors [...] Comments:Accepted at NeurIPS 2020\nSubjects:Computation and Language (cs.CL); Machine Learning (cs.LG)\nCite as:arXiv:2005.11401 [cs.CL]\n(or arXiv:2005.11401v4 [cs.CL] for this version)\n\nFocus to learn more\n\n arXiv-issued DOI via DataCite\n\nSubmission history\n\n From: Patrick Lewis [view email] \n\n( Fri, 22 May 2020 21:34:34 UTC (698 KB)\n\n( Mon, 7 Dec 2020 16:23:06 UTC (767 KB)\n\n( Mon, 29 Mar 2021 10:12:16 UTC (767 KB)\n\n[v4] Mon, 12 Apr 2021 15:42:18 UTC (767 KB)\n\n | next>\")\n\nnew | recent | 2020-05\n\n Change to browse by: \n\ncs\n\ncs.LG\n\n### References & Citations\n\n   NASA ADS\n   Google Scholar\n   Semantic Scholar\n\n### 13 blog links\n\n (what is this?) \n\n### DBLP - CS Bibliography\n\nlisting | bibtex\n\nEthan Perez\n\nAleksandra Piktus\n\nFabio Petroni\n\nVladimir Karpukhin\n\nNaman Goyal\n\n… [...] Fabio Petroni\n\nVladimir Karpukhin\n\nNaman Goyal\n\n…\n\nexport BibTeX citation Loading...\n\nBibTeX formatted citation\n\n×\n\nData provided by: _\n\n- [x] Connected Papers Toggle \n\nConnected Papers _(What is Connected Papers?)_\n\n- [x] Litmaps Toggle \n\nLitmaps _(What is Litmaps?)_\n\n- [x] scite.ai Toggle \n\nscite Smart Citations _(What are Smart Citations?)_\n\nCode, Data, Media \n\nCode, Data and Media Associated with this Article\n\n- [x] alphaXiv Toggle \n\nalphaXiv _(What is alphaXiv?)_\n\n- [x] Links to Code Toggle \n\nCatalyzeX Code Finder for Papers _(What is CatalyzeX?)_\n\n- [x] DagsHub Toggle \n\nDagsHub _(What is DagsHub?)_\n\n- [x] GotitPub Toggle \n\nGotit.pub _(What is GotitPub?)_\n\n- [x] Huggingface Toggle \n\nHugging Face _(What is Huggingface?)_\n\n- [x] Links to Code Toggle"
    },
    {
      "url": "https://www.glean.com/blog/rag-retrieval-augmented-generation",
      "title": "What is Retrieval Augmented Generation(RAG) in 2025? - Glean",
      "snippet": "Research papers and articles\n\n \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\" by P. Lewis et al.\n \"Language Models are Few-Shot Learners\" by T. Brown et al., focusing on GPT-3 and its capabilities.\n\nWebsites\n\n Hugging Face Model Hub offers pre-trained models and datasets.\n Google AI Blog provides the latest updates on Google's AI research.\n\nTutorials and code repositories\n\n The Hugging Face Transformers library includes RAG implementation.\n Official PyTorch Tutorial offers guidance on implementing neural networks.\n\nOnline courses and lectures\n\n Coursera and edX list AI and ML courses that often cover state-of-the-art techniques.\n Stanford University's NLP course (CS224N) lectures available on YouTube.\n\nForums and community groups [...] ### Research trends\n\nResearchers are focusing on improving the interface between retrieval and generation components in RAG models. This involves enhancing the models' capacity to selectively source and integrate relevant information from extensive databases. Investigations into more sophisticated retrieval mechanisms, such as bi-directional retrieval and the use of reinforcement learning to optimize query strategies, are underway.\n\n Reinforcement learning: Optimization of retrieval based on model feedback\n Bi-directional retrieval: Simultaneous forward and backward information look-up\n\n### Technological advancements [...] Build generative AI experiencesGlean Protect\n\nSafely scale AI at workSecurity\n\nSecure by design from day oneAgentic Engine\n\nPlan & adapt over company context\n\nGLEAN WHERE YOU WORK\n\nGlean in SlackGlean in Microsoft TeamsGlean in ZoomGlean in Service CloudGlean in ServiceNowGlean in ZendeskGlean in GitHubGlean in MiroBrowser Extension\n\nSign in\n\nDEPARTMENTS\n\nAll TeamsEngineeringCustomer ServiceSalesMarketingB2B MarketingB2C MarketingPeopleIT\n\nINDUSTRIES\n\nRetailFinancial ServicesHigher EducationHealthcareGovernment\n\nSign in\n\nJoel McKelvey\n\nHead of Solutions, Glean\n\nAbdullah Haydar\n\nDirector of Engineering, LinkedIn\n\nWebinar\n\nAI Powered Engineering\n\nExpert insights and actionable strategies for accelerating developer productivity.\n\nWatch now\n\nEXPLORE\n\nBlogPrompt LibraryGuidesProduct Videos"
    }
  ]
}