{
  "metadata": {
    "key": "search:general:6:long text performance challenges in retrieval augmented generation systems",
    "created": 1769979835.769257,
    "ttl": 86400
  },
  "data": [
    {
      "url": "https://arxiv.org/html/2503.00751v1",
      "title": "RaPID: Efficient Retrieval-Augmented Long Text Generation ... - arXiv",
      "snippet": "Despite its promising results, long text generation based on multi-agent disscussion faces three challenges as follows:\n(1) writing intentions are often general and ambiguous\n, but agents are constrained by the internal knowledge of LLMs, making it highly possible to misinterpret the writing intention from the very beginning and generate hallucinations.\n(2) agent interactions lack self-correction mechanisms, leading to a failure to detect hallucinations and allowing ineffective discussions to perpetuate errors unchecked.\n(3) long texts inherently involve intricate long-range dependencies and complex logical structures. Consequently, maintaining consistency and accuracy throughout the generation process remains challenging. [...] Generating knowledge-intensive and comprehensive long texts, such as encyclopedia articles, remains significant challenges for Large Language Models. It requires not only the precise integration of facts but also the maintenance of thematic coherence throughout the article.\nExisting methods, such as direct generation and multi-agent discussion, often struggle with issues like hallucinations, topic incoherence, and significant latency.\nTo address these challenges, we propose RaPID,\nan efficient retrieval-augmented long text generation framework. [...] RaPID: Efficient Retrieval-Augmented Long Text Generation with\n  \nWriting Planning and Information Discovery\n\n## 1 Introduction\n\nRefer to caption\n\nLarge Language Models (LLMs) have showcased significant proficiency in handling various natural language tasks (Chen et al., 2024a; Hu et al., 2024; Chen et al., 2024b), achieving near-human performance in tasks like summarizing lengthy documents (Liu et al., 2024b) and crafting poetry (Yu et al., 2024).\nDespite these achievements, generating long and knowledge-intensive texts (e.g., encyclopedia articles) faces significant challenge (Shen et al., 2023). Such tasks demand not only the seamless integration of facts and narratives over extensive text but also the maintenance of thematic and stylistic consistency throughout the document."
    },
    {
      "url": "https://arxiv.org/html/2410.05983v1",
      "title": "Long-Context LLMs Meet RAG: Overcoming Challenges for ... - arXiv",
      "snippet": "This paper investigates the impact of increasing the number of retrieved passages on the performance of long-context LLMs in retrieval-augmented generation (RAG) systems. Contrary to expectations, we observe that performance initially improve but then degrade as more passages are included. This phenomenon is attributed to the detrimental influence of retrieved \"hard negatives\". To mitigate this issue, we propose and evaluate three solutions: training-free retrieval reordering, RAG-specific implicit LLM fine-tuning, and RAG-oriented LLM fine-tuning with intermediate reasoning. A systematic analysis of the training-based methods explores the effects of data distribution, retriever for training, and training context length. Interesting future directions include exploring (automated) position [...] Observations. Figure 9 presents the following key observations similar to that in Section 3.1: 1) Strong Retriever (e5): Across all LLMs, increasing the number of retrieved passages initially enhances performance, but subsequently results in either a sharp decline or a plateau. 2) Weak Retriever (BM25): Performance generally shows a continuous improvement or a slighter decrease as the number of retrieved passages increases. While these observations may appear counter-intuitive - given that one might expect monotonic improvements due to higher recall (i.e., a greater chance of retrieving relevant information) - the inclusion of additional documents can reduce precision, with irrelevant or misleading passages detracting LLMs from overall performance. [...] Large language models (LLMs) can be prone to hallucinations especially at knowledge-intensive tasks (Zhao et al., 2023; Huang et al., 2023; Augenstein et al., 2023). Retrieval-augmented generation (RAG) addresses this by incorporating external knowledge sources to provide accurate and relevant information (Gao et al., 2023). Traditional RAG systems comprise a retriever to identify relevant information and a generator to synthesize the answer (Zhao et al., 2024; Zhu et al., 2021). While previous research focused on improving either the retriever (Karpukhin et al., 2020; Izacard et al., 2021; Wang et al., 2022) or the generator (Dong et al., 2022; Liu et al., 2024; Agarwal et al., 2024) in isolation, we take a holistic approach. Conducting comprehensive analyses of the entire RAG system, we"
    },
    {
      "url": "https://labelstud.io/blog/rag-fundamentals-challenges-and-advanced-techniques/",
      "title": "RAG: Fundamentals, Challenges, and Advanced Techniques",
      "snippet": "However, even with well-structured data, retrieval systems aren’t static—they need to adapt and improve over time. One of the biggest challenges in RAG is incorporating feedback to refine chunking, retrieval, and response quality. Improving RAG requires continuous feedback mechanisms that monitor performance and adjust retrieval processes accordingly.\n\nSeveral key approaches help refine retrieval quality over time:\n\nA recent paper, “Seven Failure Points When Engineering a Retrieval Augmented Generation System,” outlines some of the most common pitfalls when designing and tuning RAG architectures. We’ll explore these challenges—and how to address them—in a future blog.\n\n## Beyond Text: Expanding RAG’s Capabilities [...] # RAG: Fundamentals, Challenges, and Advanced Techniques\n\nRetrieval-Augmented Generation (RAG) has become a go-to method for improving Large Language Models (LLMs). RAG helps mitigate issues like outdated information, high performance costs, and hallucinations, making it ideal for many real world applications. [...] ## Conclusion\n\nRAG is a powerful technique for enhancing LLM performance, but as we’ve explored, implementing it effectively requires ongoing refinement. From ranking and retrieval optimization to handling complex queries and multimodal data, each challenge highlights the need for continuous improvement in how we manage and retrieve information.\n\nOne of the most critical areas for improvement is incorporating human feedback into the RAG pipeline. No matter how advanced retrieval techniques become, errors in chunking, ranking, or context selection can introduce misinformation and reduce accuracy. By integrating feedback loops, error detection, and adaptive learning methods, we can create RAG systems that better serve real-world applications."
    },
    {
      "url": "https://www.premai.io/blog/rag-vs-long-context-llms-approaches-for-real-world-applications",
      "title": "RAG vs Long-Context LLMs: Approaches for Real-World Applications",
      "snippet": "Adaptive Control of Retrieval-Augmented Generation for Large Language Models Through Reflective TagsWhile retrieval-augmented generation (RAG) enhances large language models (LLMs), it also introduces challenges that can impact accuracy and performance. In practice, RAG can obscure the intrinsic strengths of LLMs. Firstly, LLMs may become too reliant on external retrieval, underutilizing their own knowledge and reasoning, which can diminish responsiveness. Secondly, RAG may introduce irrelevant or low-quality data, adding noise that disrupts generation, especially with complex tasks. This paper proposes an RAG framework that uses reflective tags to manage retrieval, evaluating documents in parallel and applying the chain-of-thought (CoT) technique for step-by-step generation. The model [...] ### Failure Modes and Challenges\n\n#### RAG Challenges\n\nRAG faces several notable challenges, particularly related to the quality and relevance of the retrieved information. The efficiency of RAG is highly dependent on the retriever's accuracy; if the retrieval process selects irrelevant or incorrect data, the quality of the generated response is compromised. This issue becomes especially pronounced when dealing with \"hard negatives\"—documents that are contextually similar but do not provide the correct answer. Such instances can lead to increased noise in the generation process, ultimately affecting performance. [...] ### Real-World Applications of RAG and Long-Context LLMs\n\nTo illustrate the real-world applications of Retrieval-Augmented Generation (RAG) and Long-Context LLMs, the following figure shows the effectiveness of retrieval reordering in various RAG configurations. It demonstrates how retrieval ordering can enhance performance, particularly as the number of retrieved passages increases. This visualization helps provide insight into the optimization potential of RAG systems in real-world scenarios, such as healthcare, finance, and legal applications.\n\nSource:Long-Context LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG"
    },
    {
      "url": "https://www.allganize.ai/en/blog/retriever-optimization-strategies-for-successful-rag",
      "title": "Retriever Optimization Strategies for Successful RAG - Allganize",
      "snippet": "Chunking Text into Appropriate Sizes\n\nOne of the first challenges in constructing retrieval for RAG is determining the size of the chunks. Since it’s difficult to process the entire text of a document at once, we break it into smaller units, called \"chunks.\"\n\nFor example, imagine splitting the context into 300-token chunks. While this method divides the text into manageable pieces, there is a risk of losing surrounding information, which can lead to less accurate answers. Ideally, retrieval performance would be perfect, capturing all the relevant context and generating precise responses, but realistically, achieving 100% retrieval accuracy is difficult."
    },
    {
      "url": "https://arxiv.org/html/2411.08438v1",
      "title": "Towards Optimizing a Retrieval Augmented Generation using Large ...",
      "snippet": "### 4.3. Generation Phase\n\nWe use In-context learning, which allows LLMs to learn new tasks on run-time through examples passed as part of the input prompt. We explore three-shot learning in our experiments to evaluate if it would improve the performance of an RAG framework. A considerable challenge was selecting three Question-Answer pairs that exhibit a broad semantic diversity to prevent model bias towards the domains of the examples provided.\n\n### 4.4. Evaluation\n\nThe growing importance of RAG-based applications requires a detailed evaluation method that separates Retrieval Quality and Generation Quality.\n\n#### 4.4.1. HIT Rate [...] Retrieval-augmented generation systems can be deployed for a wide variety of NLP tasks involving text generation. The most popular is open-domain question answering (Siriwardhana et al., 2023), where the task is to answer a given question with no provided context, i.e., the system has to first search through large knowledge bases in order to find an answer. Beyond QA, RAG can also be used for generative tasks like machine translation by retrieving example sentences from a corpus in target language (Cai et al., 2021), or for dialogue generation by guiding the conversation with retrieved exemplar responses from previous dialogues (Gupta et al., 2021). Applying AI and NLP methods to help education and students has been widely studied (Zhang and Aslan, 2021; Kasneci et al., 2023). Example use [...] Cai et al. (2022)  Deng Cai, Yan Wang, Lemao Liu, and Shuming Shi. 2022.  Recent advances in retrieval-augmented text generation. In Proceedings of the 45th international ACM SIGIR conference on research and development in information retrieval.\n Chen et al. (2017)  Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017.  Reading Wikipedia to Answer Open-Domain Questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Vancouver, Canada."
    }
  ]
}