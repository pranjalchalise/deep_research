{
  "metadata": {
    "key": "search:general:6:How is langchain defined in the context of its updates?",
    "created": 1770002930.6829069,
    "ttl": 86400
  },
  "data": [
    {
      "url": "https://cloud.google.com/use-cases/langchain",
      "title": "What Is LangChain? Examples and definition - Google Cloud",
      "snippet": "## How does LangChain work?\n\nLangChain works by \"chaining\" together different components to create a cohesive workflow for LLM-powered applications. This modular approach breaks down complex language-based AI systems into reusable parts. When a user submits a query, LangChain can process this input through a series of steps.\n\nFor example, a typical workflow might involve: [...] # What is LangChain?\n\nLangChain is an open-source orchestration framework that simplifies building applications with large language models (LLMs). It provides tools and components to connect LLMs with various data sources, enabling the creation of complex, multi-step workflows.\n\nAvailable as libraries in Python and JavaScript, LangChain helps developers enhance LLM capabilities beyond text generation by linking them to external data and computation. This helps facilitate the development of advanced AI applications like intelligent chatbots, sophisticated question-answering systems, and automated data analysis tools.\n\nGet started for free\n\nBuild AI-powered apps on Vertex AI with LangChain\n\n# LangChain and AI [...] ### Agents\n\n Multi-agent communication and interaction: LangChain supports the creation and deployment of multiple language understanding agents, enabling complex collaboration and coordination between models.\n Centralized agent coordination: Provides centralized coordination and supervision for language understanding agents, ensuring efficient task distribution and resource management within multi-agent systems.\n\n### Memory\n\n Extensible external memory: Incorporates custom external memory modules, allowing users to extend and customize memory management to meet specific requirements.\n Adaptive context allocation: LangChain uses adaptive algorithms for memory allocation, optimizing resource utilization for efficient context storage and retrieval.\n\n### LangChain applications and examples"
    },
    {
      "url": "https://www.ibm.com/think/topics/langchain",
      "title": "What Is LangChain? | IBM",
      "snippet": "LangChain is essentially a library of abstractions for Python and Javascript, representing common steps and concepts necessary to work with language models. These modular components‚Äîlike functions and object classes‚Äîserve as the building blocks of generative AI programs. They can be ‚Äúchained‚Äù together to create applications, minimizing the amount of code and fine understanding required to execute complex NLP tasks. Though LangChain‚Äôs abstracted approach may limit the extent to which an expert programmer can finely customize an application, it empowers specialists and newcomers alike to quickly experiment and prototype.\n\n### Importing language models [...] My IBM\n\nLog in\n\nSubscribe\n\n# What is LangChain?\n\n## Authors\n\nDave Bergmann\n\nSenior Staff Writer, AI Models\n\nIBM Think\n\nCole Stryker\n\nStaff Editor, AI Models\n\nIBM Think\n\n## LangChain overview\n\nLangChain is an open source orchestration framework for application development using large language models (LLMs). Available in both Python- and Javascript-based libraries, LangChain‚Äôs tools and APIs simplify the process of building LLM-driven applications like chatbots and AI agents. [...] LangChain serves as a generic interface for nearly any LLM, providing a centralized development environment to build LLM applications and integrate them with external data sources and software workflows. LangChain‚Äôs module-based approach allows developers and data scientists to dynamically compare different prompts and even different foundation models with minimal need to rewrite code. This modular environment also allows for programs that use multiple LLMs: for example, an application that uses one LLM to interpret user queries and another LLM to author a response."
    },
    {
      "url": "https://docs.langchain.com/oss/python/concepts/context",
      "title": "Context overview - Docs by LangChain",
      "snippet": "1. By mutability:\n\n Static context: Immutable data that doesn‚Äôt change during execution (e.g., user metadata, database connections, tools)\n Dynamic context: Mutable data that evolves as the application runs (e.g., conversation history, intermediate results, tool call observations)\n\n1. By lifetime:\n\n Runtime context: Data scoped to a single run or invocation\n Cross-conversation context: Data that persists across multiple conversations or sessions\n\nRuntime context refers to local context: data and dependencies your code needs to run. It does not refer to:\n\n The LLM context, which is the data passed into the LLM‚Äôs prompt.\n The ‚Äúcontext window‚Äù, which is the maximum number of tokens that can be passed to the LLM. [...] | Context type | Description | Mutability | Lifetime | Access method |\n ---  --- \n| Static runtime context | User metadata, tools, db connections passed at startup | Static | Single run | `context` argument to `invoke`/`stream` |\n| Dynamic runtime context (state) | Mutable data that evolves during a single run | Dynamic | Single run | LangGraph state object |\n| Dynamic cross-conversation context (store) | Persistent data shared across conversations | Dynamic | Cross-conversation | LangGraph store |\n\n## ‚Äã Static runtime context\n\nStatic runtime context represents immutable data like user metadata, tools, and database connections that are passed to an application at the start of a run via the `context` argument to `invoke`/`stream`. This data does not change during execution.\n\nCopy [...] Runtime context is a form of dependency injection and can be used to optimize the LLM context. It lets to provide dependencies (like database connections, user IDs, or API clients) to your tools and nodes at runtime rather than hardcoding them. For example, you can use user metadata in the runtime context to fetch user preferences and feed them into the context window.\n\nLangGraph provides three ways to manage context, which combines the mutability and lifetime dimensions:"
    },
    {
      "url": "https://medium.com/around-the-prompt/what-is-langchain-and-why-should-i-care-as-a-developer-b2d952c42b28",
      "title": "What is Langchain and why should I care as a developer? - Medium",
      "snippet": "A quick note on truncation: this is the process of taking the message history and continually narrowing it down in order to stay within the language models context window. There‚Äôs lots of different ways of doing it, and in fact ChatGPT has a custom approach, but there is no silver bullet here since you will always end up having to omit important info you care about. This is why Embeddings and memory through langchain can be so useful.\n\n### Comparison tools üîç [...] Langchain makes creating agents using large language models simple through their agents API. Developers can use OpenAI functions or other means of executing tasks to enable to language model to take actions. [...] ### Why does langchain exist? ü§î\n\nSimply put, there are many rough edges to working with languages models today. The entire ecosystem is still developing so for developers, there is generally a lack of sufficient tooling to make production deployments of languages models.\n\nTasks like prompt chaining, logging, call backs, persistent memory, and efficient connections to multiple data sources come standard out of the box with langchain.\n\nLangchain also provides a model agnostic toolset that enables companies and developers to explore multiple LLM offerings and test what works best for their use cases. The best part is that you can do this in a single interface instead of having to linearly scale the size of a code base which each additional provider you try to support."
    },
    {
      "url": "https://docs.langchain.com/oss/python/langchain/context-engineering",
      "title": "Context engineering in agents - Docs by LangChain",
      "snippet": "Transient vs Persistent Message Updates:The examples above use `wrap_model_call` to make transient updates - modifying what messages are sent to the model for a single call without changing what‚Äôs saved in state.For persistent updates that modify state (like the summarization example in Life-cycle Context), use life-cycle hooks like `before_model` or `after_model` to permanently update the conversation history. See the middleware documentation for more details.\n\n### ‚Äã Tools\n\nTools let the model interact with databases, APIs, and external systems. How you define and select tools directly impacts whether the model can complete tasks effectively.\n\n#### ‚Äã Defining tools [...] context_schema=Context,  context_schema =Context, store=InMemoryStore()  store =InMemoryStore())) [...] Configure output format based on Runtime Context like user role or environment:\n\nCopy"
    },
    {
      "url": "https://www.langchain.com/langchain",
      "title": "Build agents faster, your way - LangChain",
      "snippet": "Docs\n\nCompany\n\nAboutCareers\n\nPricing\n\nGet a demo\n\nTry LangSmith\n\nBuild agents faster, your way\n\nLangChain is an open source framework with a pre-built agent architecture and integrations for any model or tool ‚Äî so you can build agents that adapt as fast as the ecosystem evolves.\n\nGet started\n\nWhy use LangChain?\n\nShip fast with proven agent patterns\n\nBuild agents in minutes with templates for common use cases. create_agent provides a proven ReAct pattern on LangGraph's durable runtime.\n\nOpen and neutral by design\n\nSwap models, tools, and databases without rewriting your application. With 1000+ integrations, you can future-proof your stack as AI advances, with no vendor lock-in.\n\nCustomize without complexity [...] How do I use LangChain with LangSmith?\n\nLangChain is an open source framework with pre-built agent architectures and as integrations to models, tools, and databases to start building agents quickly. The LangChain framework integrates seamlessly with LangSmith, our platform for agent observability, evaluation, and deployment ‚Äî you can set just one environment variable to get started.\n\nReady to start shipping reliable agents faster?\n\nGet started with tools from the LangChain product suite for every step of the agent development lifecycle.\n\nTalk to salesSign up for free\n\nProducts\n\nLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith Deployment\n\nResources\n\nGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsSupport\n\nCompany"
    }
  ]
}