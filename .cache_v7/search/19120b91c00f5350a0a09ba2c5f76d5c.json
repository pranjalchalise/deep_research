{
  "metadata": {
    "key": "search:general:6:retrieval augmented generation definition and principles",
    "created": 1769979727.560163,
    "ttl": 86400
  },
  "data": [
    {
      "url": "https://en.wikipedia.org/wiki/Retrieval-augmented_generation",
      "title": "Retrieval-augmented generation",
      "snippet": "Wikipedia\nThe Free Encyclopedia\n\n## Contents\n\n# Retrieval-augmented generation\n\nRetrieval-augmented generation (RAG) is a technique that enables large language models (LLMs) to retrieve and incorporate new information from external data sources. With RAG, LLMs do not respond to user queries until they refer to a specified set of documents. These documents supplement information from the LLM's pre-existing training data. This allows LLMs to use domain-specific and/or updated information that is not available in the training data. For example, this helps LLM-based chatbots access internal company data or generate responses based on authoritative sources. [...] RAG improves large language models (LLMs) by incorporating information retrieval before generating responses. Unlike LLMs that rely on static training data, RAG pulls relevant text from databases, uploaded documents, or web sources. According to Ars Technica, \"RAG is a way of improving LLM performance, in essence by blending the LLM process with a web search or other document look-up process to help LLMs stick to the facts.\" This method helps reduce AI hallucinations, which have caused chatbots to describe policies that don't exist, or recommend nonexistent legal cases to lawyers that are looking for citations to support their arguments. [...] ## Process\n\nRetrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating an information-retrieval mechanism that allows models to access and utilize additional data beyond their original training set. Ars Technica notes that \"when new information becomes available, rather than having to retrain the model, all that’s needed is to augment the model’s external knowledge base with the updated information\" (\"augmentation\"). IBM states that \"in the generative phase, the LLM draws from the augmented prompt and its internal representation of its training data to synthesize an engaging answer tailored to the user in that instant\".\n\n### RAG key stages"
    },
    {
      "url": "https://www.ibm.com/think/topics/retrieval-augmented-generation",
      "title": "What is RAG (Retrieval Augmented Generation)? - IBM",
      "snippet": "# What is retrieval augmented generation (RAG)?\n\n## What is retrieval augmented generation (RAG)?\n\nRetrieval augmented generation, or RAG, is an architecture for optimizing the performance of an artificial intelligence (AI) model by connecting it with external knowledge bases. RAG helps large language models (LLMs) deliver more relevant responses at a higher quality.\n\nGenerative AI (gen AI) models are trained on large datasets and refer to this information to generate outputs. However, training datasets are finite and limited to the information the AI developer can access—public domain works, internet articles, social media content and other publicly accessible data. [...] Standard LLMs source information from their training datasets. RAG adds an information retrieval component to the AI workflow, gathering relevant information and feeding that to the generative AI model to enhance response quality and utility.\n\nRAG systems follow a five-stage process:\n\n1. The user submits a prompt.\n2. The information retrieval model queries the knowledge base for relevant data.\n3. Relevant information is returned from the knowledge base to the integration layer.\n4. The RAG system engineers an augmented prompt to the LLM with enhanced context from the retrieved data.\n5. The LLM generates an output and returns an output to the user."
    },
    {
      "url": "https://www.mckinsey.com/featured-insights/mckinsey-explainers/what-is-retrieval-augmented-generation-rag",
      "title": "What is retrieval-augmented generation (RAG)?",
      "snippet": "Retrieval-augmented generation, or RAG, is a process applied to LLMs to make their outputs more relevant in specific contexts. RAG allows LLMs to access and reference information outside the LLMs own training data, such as an organization’s specific knowledge base, before generating a response—and, crucially, with citations included. This capability enables LLMs to produce highly specific outputs without extensive fine-tuning or training, delivering some of the benefits of a custom LLM at considerably less expense. [...] Once the library is stocked and indexed, the “retrieval” phase begins. Whenever a user asks a question on a specific topic, the librarian uses the index to locate the most relevant books. The selected books are then scanned for relevant content, which is carefully extracted and synthesized into a concise output. The original question informs the initial research and selection process, guiding the librarian to present only the most pertinent and accurate information in response. This process might involve summarizing key points from multiple sources, quoting authoritative texts, or even generating new content based on the insights that can be gleaned from the library’s resources. [...] Through these ingestion and retrieval phases, RAG can generate highly specific outputs that would be impossible for traditional LLMs to produce on their own. The stocked library and index provide a foundation for the librarian to select and synthesize information in response to a query, leading to a more relevant and thus more helpful answer.\n\nIn addition to accessing a company’s internal “library,” many RAG implementations can query external systems and sources in real time. Examples of such searches include the following:"
    },
    {
      "url": "https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval",
      "title": "A Practitioners Guide to Retrieval Augmented Generation (RAG)",
      "snippet": "Adding ranking to the retrieval pipeline, either using a cross-encoder or a hybrid model that performs both retrieval and ranking (e.g., ColBERT ).\n Finetuning the embedding model for dense retrieval over human-collected relevance data (i.e., pairs of input prompts with relevant/irrelevant passages).\n Finetuning the LLM generator over examples of high-quality outputs so that it learns to better follow instructions and leverage useful context.\n Using LLMs to augment either the input prompt or the textual chunks with extra synthetic data to improve retrieval. [...] to retrieve and leverage knowledge via its pretrained components. [...] Many RAG applications use pure vector search to find relevant textual chunks, but we can create a much better retrieval pipeline by re-purposing existing approaches from AI-powered search. Namely, we can augment dense retrieval with a lexical (or keyword-based) retrieval component, forming a hybrid search algorithm. Then, we can add a fine-grained re-ranking step—either with a cross-encoder or a less expensive component (e.g., ColBERT )—to sort candidate chunks based on relevance; see above for a depiction."
    },
    {
      "url": "https://www.box.com/resources/what-is-retrieval-augmented-generation",
      "title": "What is retrieval-augmented generation (RAG)? - Box",
      "snippet": "1. Retrieval: A system, often based on semantic search, identifies relevant documents from your cloud data storage or any other knowledge source\n2. Augmentation: The retrieved information is combined with an LLM’s pre-trained data to create a more informative prompt, integrating the LLM with additional context\n3. Generation: The LLM generates an answer to the query, using the understanding of language and the information from your documents to create a more relevant response [...] Retrieval-augmented generation (RAG) is a technique in natural language processing (NLP) that improves text generation by incorporating data from databases, digital asset libraries, or other knowledge sources. Imagine a legal team searching for precedents in a vast collection of case law documents. Through RAG, they could quickly get a summary of a retrieved document and identify key points that they can validate and use to support their arguments.\n\nThanks to artificial intelligence (AI), this process happens in seconds. RAG uses AI-driven large language models (LLMs) — trained on massive datasets of text and code — to generate accurate and contextual content based on sources like documents, spreadsheets, and presentations. [...] Box AI: Integrate advanced AI models into your content management platform to locate documents in real time and summarize extensive contracts instantly\n AI principles: Have complete control over your use of AI with transparent and responsible guidelines that protect your content quality, confidentiality, and safety\n Box Hubs: Perform cross-document searches and receive reliable and comprehensive answers about existing topics so you can make more informed decisions\n Box Notes: Create emails, meeting agendas, and guides from scratch or based on existing models, ensuring consistency across your documents\n Enterprise-grade security: Protect your valuable information with AES 256-bit encryption, advanced authentication, device trust, granular access controls, and more"
    },
    {
      "url": "https://www.promptingguide.ai/research/rag",
      "title": "Retrieval Augmented Generation (RAG) for LLMs",
      "snippet": "Adaptive retrieval tailors the retrieval process to specific demands by determining optimal moments and content for retrieval. Notable approaches that leverage this method include FLARE (opens in a new tab) and Self-RAG (opens in a new tab). [...] Augmentation Process: For many problems (e.g., multi-step reasoning), a single retrieval isn't enough so a few methods have been proposed: [...] | A framework to generate context-relevant and knowledge-grounded dialogues with a knowledge graph (KG). It first retrieves the relevant subgraph from the KG, and then enforces consistency across facts by perturbing their word embeddings conditioned by the retrieved subgraph. Then, it utilizes contrastive learning to ensure that the generated texts have high similarity to the retrieved subgraphs. | Knowledge Graph-Augmented Language Models for Knowledge-Grounded Dialogue Generation (opens in a new tab) | May 2023 |"
    }
  ]
}