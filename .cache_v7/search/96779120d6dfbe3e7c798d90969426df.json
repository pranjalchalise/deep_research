{
  "metadata": {
    "key": "search:general:6:How is Langchain defined in the context of its new features?",
    "created": 1770002870.6938229,
    "ttl": 86400
  },
  "data": [
    {
      "url": "https://www.ibm.com/think/topics/langchain",
      "title": "What Is LangChain? | IBM",
      "snippet": "LangChain is essentially a library of abstractions for Python and Javascript, representing common steps and concepts necessary to work with language models. These modular components—like functions and object classes—serve as the building blocks of generative AI programs. They can be “chained” together to create applications, minimizing the amount of code and fine understanding required to execute complex NLP tasks. Though LangChain’s abstracted approach may limit the extent to which an expert programmer can finely customize an application, it empowers specialists and newcomers alike to quickly experiment and prototype.\n\n### Importing language models [...] LangChain serves as a generic interface for nearly any LLM, providing a centralized development environment to build LLM applications and integrate them with external data sources and software workflows. LangChain’s module-based approach allows developers and data scientists to dynamically compare different prompts and even different foundation models with minimal need to rewrite code. This modular environment also allows for programs that use multiple LLMs: for example, an application that uses one LLM to interpret user queries and another LLM to author a response. [...] My IBM\n\nLog in\n\nSubscribe\n\n# What is LangChain?\n\n## Authors\n\nDave Bergmann\n\nSenior Staff Writer, AI Models\n\nIBM Think\n\nCole Stryker\n\nStaff Editor, AI Models\n\nIBM Think\n\n## LangChain overview\n\nLangChain is an open source orchestration framework for application development using large language models (LLMs). Available in both Python- and Javascript-based libraries, LangChain’s tools and APIs simplify the process of building LLM-driven applications like chatbots and AI agents."
    },
    {
      "url": "https://docs.langchain.com/oss/python/concepts/context",
      "title": "Context overview - Docs by LangChain",
      "snippet": "| Context type | Description | Mutability | Lifetime | Access method |\n ---  --- \n| Static runtime context | User metadata, tools, db connections passed at startup | Static | Single run | `context` argument to `invoke`/`stream` |\n| Dynamic runtime context (state) | Mutable data that evolves during a single run | Dynamic | Single run | LangGraph state object |\n| Dynamic cross-conversation context (store) | Persistent data shared across conversations | Dynamic | Cross-conversation | LangGraph store |\n\n## ​ Static runtime context\n\nStatic runtime context represents immutable data like user metadata, tools, and database connections that are passed to an application at the start of a run via the `context` argument to `invoke`/`stream`. This data does not change during execution.\n\nCopy [...] 1. By mutability:\n\n Static context: Immutable data that doesn’t change during execution (e.g., user metadata, database connections, tools)\n Dynamic context: Mutable data that evolves as the application runs (e.g., conversation history, intermediate results, tool call observations)\n\n1. By lifetime:\n\n Runtime context: Data scoped to a single run or invocation\n Cross-conversation context: Data that persists across multiple conversations or sessions\n\nRuntime context refers to local context: data and dependencies your code needs to run. It does not refer to:\n\n The LLM context, which is the data passed into the LLM’s prompt.\n The “context window”, which is the maximum number of tokens that can be passed to the LLM. [...] Docs by LangChain home page\n\nLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContribute\n\n Learn\n\n##### Tutorials\n\n##### Conceptual overviews\n\n Component architecture\n Memory\n Context\n Graph API\n Functional API\n\n##### Additional resources\n\n LangChain Academy\n Case studies\n Get help\n\n Static runtime context\n Dynamic runtime context\n Dynamic cross-conversation context\n See also\n\nConceptual overviews\n\n# Context overview\n\nContext engineering is the practice of building dynamic systems that provide the right information and tools, in the right format, so that an AI application can accomplish a task. Context can be characterized along two key dimensions:\n\n1. By mutability:"
    },
    {
      "url": "https://focused.io/lab/langchain-under-the-hood-5-features-we-rely-on-daily",
      "title": "LangChain Under the Hood: 5 Features We Rely On Daily | Focused",
      "snippet": "To do this, we create a RunnablePassthrough object that passes the user input to the retriever which retrieves the context data and formats it before finally injecting the resulting content into our prompt template’s context variable.\n\nWith the abstractions LangChain provides, RAG functionality can be achieved easily without the need for fussing over the nuances of integrating with one vector store vs another or planning an intensive migration if, or more likely when, you decide that you want to try out the hot new vector database. [...] To do this, we create a RunnablePassthrough object that passes the user input to the retriever which retrieves the context data and formats it before finally injecting the resulting content into our prompt template’s context variable.\n\nWith the abstractions LangChain provides, RAG functionality can be achieved easily without the need for fussing over the nuances of integrating with one vector store vs another or planning an intensive migration if, or more likely when, you decide that you want to try out the hot new vector database. [...] To do this, we create a RunnablePassthrough object that passes the user input to the retriever which retrieves the context data and formats it before finally injecting the resulting content into our prompt template’s context variable.\n\nWith the abstractions LangChain provides, RAG functionality can be achieved easily without the need for fussing over the nuances of integrating with one vector store vs another or planning an intensive migration if, or more likely when, you decide that you want to try out the hot new vector database."
    },
    {
      "url": "https://docs.langchain.com/oss/python/releases/langchain-v1",
      "title": "What's new in LangChain v1",
      "snippet": "create_agent ------------ The new standard for building agents in LangChain, replacing `langgraph.prebuilt.create_react_agent`.Standard content blocks ----------------------- A new `content_blocks` property that provides unified access to modern LLM features across providers.Simplified namespace -------------------- The `langchain` namespace has been streamlined to focus on essential building blocks for agents, with legacy functionality moved to `langchain-classic`.\n\nTo upgrade,\n\npip\n\nuv\n\nCopy\n\n```\npip install -U langchain\n```\n\nFor a complete list of changes, see the migration guide.\n​\n\n`create_agent` [...] Image 3: Core agent loop diagram\n\nFor more information, see Agents.\n### ​\n\nMiddleware\n\nMiddleware is the defining feature of `create_agent`. It offers a highly customizable entry-point, raising the ceiling for what you can build.Great agents require context engineering: getting the right information to the model at the right time. Middleware helps you control dynamic prompts, conversation summarization, selective tool access, state management, and guardrails through a composable abstraction.\n#### ​\n\nPrebuilt middleware [...] ​\n\nSimplified package\n\nLangChain v1 streamlines the `langchain` package namespace to focus on essential building blocks for agents. The refined namespace exposes the most useful and relevant functionality:\n### ​\n\nNamespace\n\n| Module | What’s available | Notes |\n --- \n| `langchain.agents` | `create_agent`, `AgentState` | Core agent creation functionality |\n| `langchain.messages` | Message types, content blocks, `trim_messages` | Re-exported from `langchain-core` |\n| `langchain.tools` | `@tool`, `BaseTool`, injection helpers | Re-exported from `langchain-core` |\n| `langchain.chat_models` | `init_chat_model`, `BaseChatModel` | Unified model initialization |\n| `langchain.embeddings` | `Embeddings`, `init_embeddings` | Embedding models |"
    },
    {
      "url": "https://docs.langchain.com/oss/javascript/releases/langchain-v1",
      "title": "What's new in LangChain v1",
      "snippet": "### ​\n\nBenefits\n\n   Provider agnostic: Access reasoning traces, citations, built-in tools (web search, code interpreters, etc.), and other features using the same API regardless of provider\n   Type safe: Full type hints for all content block types\n   Backward compatible: Standard content can be loaded lazily, so there are no associated breaking changes\n\nFor more information, see our guide on content blocks\n\n  \n\n​\n\nSimplified package\n\nLangChain v1 streamlines the `langchain` package namespace to focus on essential building blocks for agents. The package exposes only the most useful and relevant functionality:Most of these are re-exported from `@langchain/core` for convenience, which gives you a focused API surface for building agents.\n### ​\n\n`@langchain/classic` [...] createAgent ----------- A new standard way to build agents in LangChain, replacing `createReactAgent` from LangGraph with a cleaner, more powerful API.Standard content blocks ----------------------- A new `contentBlocks` property that provides unified access to modern LLM features across all providers.Simplified package ------------------ The `langchain` package has been streamlined to focus on essential building blocks for agents, with legacy functionality moved to `@langchain/classic`.\n\nTo upgrade,\n\nnpm\n\npnpm\n\nyarn\n\nbun\n\nCopy\n\n```\nnpm install langchain @langchain/core\n```\n\nFor a complete list of changes, see the migration guide.\n​\n\n`createAgent` [...] Image 3: Core agent loop diagram\n\nFor more information, see Agents.\n### ​\n\nMiddleware\n\nMiddleware is the defining feature of `createAgent`. It makes `createAgent` highly customizable, raising the ceiling for what you can build.Great agents require context engineering: getting the right information to the model at the right time. Middleware helps you control dynamic prompts, conversation summarization, selective tool access, state management, and guardrails through a composable abstraction.\n#### ​\n\nPrebuilt middleware"
    },
    {
      "url": "https://docs.langchain.com/oss/python/langchain/overview",
      "title": "LangChain overview",
      "snippet": "##### Agent development\n\n   LangSmith Studio\n   Test\n   Agent Chat UI\n\n##### Deploy with LangSmith\n\n   Deployment\n   Observability\n\nOn this page\n   Create an agent\n   Core benefits\n\nLangChain overview\n\nCopy page\n\nLangChain is an open source framework with a pre-built agent architecture and integrations for any model or tool — so you can build agents that adapt as fast as the ecosystem evolves\n\nCopy page [...] LangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and more. LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.We recommend you use LangChain if you want to quickly build agents and autonomous applications. Use LangGraph, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.LangChain agents are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and"
    }
  ]
}