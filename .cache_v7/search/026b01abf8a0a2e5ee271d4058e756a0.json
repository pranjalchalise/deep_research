{
  "metadata": {
    "key": "search:general:6:comparison of retrieval augmented generation and other generative models",
    "created": 1769979725.956904,
    "ttl": 86400
  },
  "data": [
    {
      "url": "https://arxiv.org/html/2502.20245v1",
      "title": "From Retrieval to Generation: Comparing Different Approaches - arXiv",
      "snippet": "In this section ,we evaluate a Retrieval-Augmented Generation (RAG) setting, where retrieval-based models (BM25, MSS-DPR) are combined with generative models (e.g., InstructGPT, GenRead). The retrieved documents serve as input to a large language model, which generates the final response. Similar to prior works Karpukhin et al. (2020b); Izacard and Grave (2020), our setup involves retrieval (R), generation (G), and hybrid models (R+G, G+R). Furthermore, we implement reranking techniques such as UPR and RankGPT to refine the document selection process. Our reader model (LLama-3.3 8B) gets the question along with its corresponding retrieved documents and returns the answer. Reader models are simply a frozen large LM (not pre-trained, fine-tuned). Table 3, shows that hybrid models (R+G, G+R) [...] ### A.3 QA performance comparison on multiple benchmarks\n\nThis section presents a detailed zero-shot evaluation of various retrieval and reranking methods within a retrieval-augmented generation (RAG) framework. The performance of these methods is assessed across three widely used question-answering benchmarks: Natural Questions (NQ), TriviaQA, and WebQuestions (WebQ). The study compares different retriever and reranker models, including BM25, Multi-Stage Search (MSS), Contriever, Dense Passage Retrieval (DPR), and hybrid approaches such as retrieval + generation (R+G) and generation + retrieval (G+R). The results are reported for multiple state-of-the-art large language models, including LLaMA-3 8B, LLaMA-3.1 8B, Gemma-2 (2B and 9B), LLaMA-2-13B, and Mistral-7B-v0.1. [...] All experiments and dataset processing were conducted using the Rankify222 framework, which provides a unified toolkit for retrieval, re-ranking, and retrieval-augmented generation Abdallah et al. (2025b).\n\n### 4.2 Retrieval and Generative Models\n\nRetrieval Models: We used five retrieval models in our experiments: BM25, a sparse vector-based method; DPR, a dense dual-encoder model that maximizes similarity between questions and relevant passages; MSS, a dense retriever pre-trained on predicting masked spans like named entities; MSS-DPR, combining MSS pre-training with DPR’s fine-tuning for improved performance; and Contriever, an unsupervised dense retriever optimized for zero-shot performance through contrastive learning."
    },
    {
      "url": "https://aclanthology.org/2025.chomps-main.6.pdf",
      "title": "[PDF] A Comprehensive Evaluation of Large Language Models for ...",
      "snippet": "We compare the performance of four extractive and six generative open-source models of varying sizes (ranging from 3.8 billion to 70 billion param-eters), and three closed-source generative models, using the Retrieval-Augmented Generation Bench-mark (RGB) dataset (Chen et al., 2024). Our results show that it is possible to replace generative mod-els with smaller extractive ones when the retrieval procedure is sufficiently accurate. Additionally, we show that replacing closed-source models with open-source alternatives—when computational re-sources allow for 70B parameter models—yields comparable accuracy and noise robustness. In sce-narios with more limited resources, 7B parameter models emerge as a promising alternative, offer-ing competitive accuracy at the expense of reduced robustness [...] {josue.caldas.v, elvis.desouza99}@gmail.com Abstract Retrieval-Augmented Generation (RAG) has emerged as an effective strategy to ground Large Language Models (LLMs) with reliable, real-time information. This paper investigates the trade-off between cost and performance by evaluating 13 LLMs within a RAG pipeline for the Question Answering (Q&A) task under noisy retrieval conditions. We assess four ex-tractive and nine generative models—spanning both open- and closed-source ones of varying sizes—on a journalistic benchmark specifically designed for RAG. By systematically varying the level of noise injected into the retrieved context, we analyze not only which models per-form best, but also their robustness to noisy in-put. Results show that large open-source gener-ative models (approx. [...] In contrast, medium-sized models, such as Meta LLaMA 3 (8B) and Nous Hermes 2 (7B), operated efficiently with a single GPU, achieving inference times of 5.28 and 5.64 seconds, respectively. Fi-nally, the largest open-source generative models, Meta LLaMA 3.1 (70B) and Nous Hermes 3 (70B), required eight GPUs to achieve competitive perfor-mance, with reduced inference times of 1.16 and 1.26 seconds, respectively.5 6 Concluding Remarks This study offers three key insights into the perfor-mance, efficiency, and practical use of extractive and generative language models for Question An-swering (Q&A) in Retrieval-Augmented Genera-tion (RAG) systems."
    },
    {
      "url": "https://www.glean.com/blog/rag-retrieval-augmented-generation",
      "title": "What is Retrieval Augmented Generation(RAG) in 2025? - Glean",
      "snippet": "Performance evaluation typically involves comparing the RAG model's outputs to baselines established by traditional generative models. Metrics such as BLEU, ROUGE, and F1 scores are used to assess the quality of the generated text in tasks like question answering and document summarization.\n\n### In what ways does Retrieval-Augmented Generation contribute to the field of Generative AI?\n\nRetrieval-Augmented Generation contributes by introducing a method for leveraging external knowledge to inform text generation. This establishes a new paradigm for creating more informative and accurate AI systems that can stay current with evolving data and information trends.\n\nBack to all stories\n\nHave questions or want a demo?\n\nWe’re here to help! Click the button below and we’ll be in touch. [...] The RAG model utilizes HuggingFace's transformers as its base by integrating a retriever-read mechanism. The retriever first queries a dataset such as Wikipedia to fetch relevant documents, which are then passed to a transformer-based generator to produce the final output.\n\n### What are the key differences between Retrieval-Augmented Generation and standard generative models?\n\nUnlike standard generative models, which rely solely on their trained parameters, RAG models employ an additional retrieval step to access external knowledge. This allows them to incorporate up-to-date and detailed information, often leading to more precise and informative outputs.\n\n### How is the performance of a Retrieval-Augmented Generation model evaluated against other models? [...] Retrieval Augmented Generation (RAG) comes as a breakthrough in natural language processing, blending the power of pre-trained language models with the vast knowledge stored in external textual databases. RAG is a framework designed to enhance language generation tasks by retrieving and conditioning on relevant documents, effectively augmenting the pool of information a model can draw from when generating text. This fusion of retrieval and generation facilitates more informed and contextually relevant outputs, particularly in question-answering and conversational AI systems."
    },
    {
      "url": "https://papers.ssrn.com/sol3/Delivery.cfm/5203514.pdf?abstractid=5203514&mirid=1",
      "title": "[PDF] Retrieval-Augmented Generation: A Comprehensive Survey of ...",
      "snippet": "Retrieval-Augmented Generation (RAG) models have emerged as a powerful paradigm in natural language processing (NLP) by effectively combining the strengths of information retrieval and generative models. These models leverage external knowledge sources to enhance the accuracy, coherence, and informativeness of generated outputs, which significantly improves the performance of a wide array of NLP tasks, such as open-domain question answering, dialogue systems, summarization, and text completion. The core idea behind RAG models is to retrieve relevant documents or knowledge from large external corpora, which are then used as context for generating more contextually relevant and factually accurate responses. In this survey, we provide an extensive overview of the retrieval-augmented [...] we provide an extensive overview of the retrieval-augmented generation paradigm, delving into the foundational principles that underpin the integration of retrieval with generation. We explore the various components of RAG systems, including the retriever, which is responsible for identifying relevant documents, and the generator, which produces text based on these retrieved documents. We also cover the different retrieval techniques, such as traditional sparse retrieval methods (e.g., BM25) and modern dense retrieval methods that utilize neural networks, such as BERT-based encoders and other pre-trained language models. By utilizing dense retrieval, RAG models can capture deeper semantic relationships between queries and documents, leading to more accurate and relevant information [...] leading to more accurate and relevant information retrieval. The survey also discusses several key advancements in RAG research, including the development of end-to-end retriever-generator training, which allows both components to be optimized jointly for better performance, and the integration of pre-trained retrieval models like ColBERT and Dense Passage Retrieval (DPR). These models, pre-trained on large corpora and fine-tuned for specific tasks, enable RAG systems to leverage vast amounts of knowledge efficiently and accurately. We examine how recent work has extended RAG models to handle cross-modal and multilingual tasks, such as combining text with images or videos, to generate richer and more context-aware outputs. Despite their success, RAG models face several challenges,"
    },
    {
      "url": "https://www.ibm.com/think/topics/retrieval-augmented-generation",
      "title": "What is RAG (Retrieval Augmented Generation)? - IBM",
      "snippet": "RAG anchors LLMs in specific knowledge backed by factual, authoritative and current data. Compared to a generative model operating only on its training data, RAG models tend to provide more accurate answers within the contexts of their external data. While RAG can reduce the risk of hallucinations, it cannot make a model error-proof.\n\n### Increased user trust\n\nChatbots, a common generative AI implementation, answer questions posed by human users. For a chatbot such as ChatGPT to be successful, users need to view its output as trustworthy. RAG models can include citations to the knowledge sources in their external data as part of their responses. [...] # What is retrieval augmented generation (RAG)?\n\n## What is retrieval augmented generation (RAG)?\n\nRetrieval augmented generation, or RAG, is an architecture for optimizing the performance of an artificial intelligence (AI) model by connecting it with external knowledge bases. RAG helps large language models (LLMs) deliver more relevant responses at a higher quality.\n\nGenerative AI (gen AI) models are trained on large datasets and refer to this information to generate outputs. However, training datasets are finite and limited to the information the AI developer can access—public domain works, internet articles, social media content and other publicly accessible data. [...] ### Research\n\nAble to read internal documents and interface with search engines, RAG models excel at research. Financial analysts can generate client-specific reports with up-to-date market information and prior investment activity, while medical professionals can engage with patient and institutional records.\n\n### Content generation\n\nThe ability of RAG models to cite authoritative sources can lead to more reliable content generation. While all generative AI models can hallucinate, RAG makes it easier for users to verify outputs for accuracy.\n\n### Market analysis and product development"
    },
    {
      "url": "https://www.glean.com/blog/hybrid-vs-rag-vector",
      "title": "Hybrid Search vs. RAG and Vector Search: Key Differences - Glean",
      "snippet": "Ever since generative AI and LLMs took center stage in the world, workers have been wondering how best to apply these transformative new tools to their workflows. However, many of them ran into similar problems while trying to integrate generative AI into enterprise environments, like privacy breaches, lack of relevance, and a need for better personalization in the results they received.\n\nTo address this, most have concluded that the answer lies in retrieval augmented generation (RAG). RAG separates knowledge retrieval from the generation process via external discovery systems like enterprise search. This enables LLMs and the responses they provide to be grounded in real, external enterprise knowledge that can be readily surfaced, traced, and referenced."
    }
  ]
}