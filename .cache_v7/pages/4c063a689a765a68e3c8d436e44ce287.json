{
  "metadata": {
    "key": "page:https://medium.com/@anicomanesh/a-gentle-introduction-to-retrieval-augmented-generation-rag-2b4ca39151bd",
    "created": 1769979955.809552,
    "ttl": 604800
  },
  "data": "A Gentle Introduction to Retrieval Augmented Generation (RAG) Arash Nicoomanesh 16 min read ¬∑ May 21, 2024 -- Listen Share Press enter or click to view image in full size Image Generated by Midjourny AI In the rapidly evolving field of natural language processing (NLP), large language models (LLMs) have demonstrated remarkable capabilities in generating coherent and contextually relevant text. However, these models often face significant challenges when tasked with producing factually accurate and domain-specific content. This is where Retrieval-Augmented Generation (RAG) emerges as a powerful paradigm, combining the strengths of information retrieval systems with the creativity of generative models. At its core, RAG operates by first retrieving relevant information from a structured or unstructured knowledge base and then using this retrieved data to ground the generative process. This approach ensures that the output is not only linguistically impressive but also informed by factual and contextual knowledge. Such a mechanism is invaluable for applications like knowledge-grounded conversational agents , question-answering systems , and domain-specific tasks in fields like healthcare, law, and finance, where accuracy is paramount. One of RAG‚Äôs defining advantages is its ability to bridge the gap between two traditionally separate technologies: retrieval systems and generative models . Retrieval systems excel at pinpointing relevant content from massive datasets, but they lack the ability to synthesize or generate human-like responses. Generative models, on the other hand, can create natural-sounding text but often suffer from ‚Äúhallucination‚Äù ‚Äî producing incorrect or misleading information. By integrating these two components, RAG ensures that responses are both contextually rich and anchored in reliable data sources. The goal of this article is to provide a gentle yet comprehensive introduction to RAG . We‚Äôll explore the theoretical foundation behind the framework, demonstrate its practical applications, and walk through hands-on code examples to show how you can build your own RAG system. Whether you‚Äôre a data scientist, developer, or AI enthusiast, this article aims to equip you with the knowledge and tools needed to unlock the potential of RAG for your projects. What is RAG? Retrieval-Augmented Generation (RAG) emerges as a novel framework addressing limitations in traditional text generation models. While models like OpenAI‚Äôs GPT excel at generating coherent, context-aware text, they often struggle with factual accuracy or precise content control. RAG bridges this gap by integrating external knowledge bases with pre-trained large language models (LLMs). This synergistic approach allows LLMs to access factual information during generation, fostering more accurate and up-to-date content creation. The resulting versatility and effectiveness position RAG as a powerful tool for the future of NLP applications, from informative chatbots and precise machine translation to content creation that seamlessly blends creativity with factual accuracy. By referencing external knowledge, RAG effectively reduces the problem of generating factually incorrect content. Its integration into LLMs has resulted in widespread adoption, establishing RAG as a key technology in advancing chatbots and enhancing the suitability of LLMs for real-world applications. In an era where NLP applications are ubiquitous, ranging from chatbots and content generation to question-answering systems and language translation, Retrieval Augmented Generation techniques offer a powerful solution to elevate the quality and reliability of these applications. Whether it‚Äôs generating informative responses to user queries, crafting content that blends creativity with accuracy, or producing multilingual translations with precision, RAG models have begun to play a pivotal role. Press enter or click to view image in full size Figure 1. RAG Cheatsheet Let‚Äôs consider an initial example to generally illustrate how RAG (Retrieval-Augmented Generation) works: Scenario: Imagine you‚Äôre using a chatbot powered by a large language model (LLM) to research a trip to Italy. You ask the chatbot: ‚ÄúWhat are some interesting historical sites to visit in Rome?‚Äù Without RAG: The LLM, relying solely on its internal knowledge, might generate a list of historical sites, but some entries could be inaccurate or irrelevant. For example, it might include a famous restaurant as a historical site or miss some key landmarks. With RAG: Information Retrieval RAG activates its information retrieval component. This component searches a vast external knowledge base (like a giant online encyclopedia) for information about historical sites in Rome. Importantly, RAG doesn‚Äôt just search for keywords like ‚ÄúRome‚Äù or ‚Äúhistorical sites.‚Äù It uses semantic similarity to understand the user‚Äôs intent, which is to find places with historical significance. 2. Informing the LLM Based on its search, RAG retrieves relevant information about prominent historical sites in Rome, such as the Colosseum, Pantheon, Roman Forum, etc. It might also include details like their historical context and significance. 3. Enhanced Generation The LLM receives the retrieved information from RAG. Now armed with both its internal knowledge and the specific details about historical sites, the LLM can generate a more accurate and informative response. For example, the chatbot might respond: ‚ÄúHere are some of the most fascinating historical sites in Rome: The Colosseum, a magnificent amphitheater where gladiatorial contests were held; the Pantheon, a former Roman temple now a church, famous for its dome; and the Roman Forum, the archeological heart of Rome, where you can explore ruins of ancient temples and government buildings.‚Äù Now we have a general information about RAG we can dive into its operation. Why Use RAG? Large language models (LLMs) have revolutionized natural language processing by generating coherent and contextually rich text. However, they are not without limitations, particularly when it comes to factual accuracy and domain-specific tasks. Retrieval-Augmented Generation (RAG) is designed to overcome these shortcomings by combining retrieval systems with generative models, creating a more reliable and accurate framework. Limitations of Standard Generative Models Hallucinations Generative models often produce text that is plausible but factually incorrect or completely fabricated. For instance, a model might confidently state that ‚ÄúThe Eiffel Tower is located in Berlin,‚Äù which is clearly wrong. Lack of Domain-Specific Knowledge While LLMs like GPT are trained on vast datasets, they may not have detailed or up-to-date knowledge in specialized domains such as medicine, law, or finance. Their understanding is limited to the training data and may not reflect recent developments. Static Knowledge Since generative models are not updated in real-time, they lack access to evolving information, such as current events or updates in scientific research. How RAG Addresses These Issues RAG enhances the capabilities of generative models by grounding their output in factual, retrieved data. Here‚Äôs how it works: Grounded Generation By retrieving contextually relevant documents or snippets from an external knowledge base, RAG ensures that the generator has access to accurate and up-to-date information. This minimizes the risk of hallucinations and improves factual consistency. Domain Adaptability RAG allows the use of specialized knowledge bases, enabling it to excel in domain-specific tasks. For example, integrating a medical database into a RAG pipeline ensures that the responses are precise and medically sound. Dynamic Knowledge Access Unlike static LLMs, RAG can query a live knowledge base or regularly updated datasets, making it a more versatile tool for applications requiring current information. Real-World Applications of RAG Customer Support: A customer support chatbot that retrieves policy details from a company‚Äôs internal knowledge base to answer specific user queries. This approach reduces inaccuracies and improves user satisfaction by providing reliable information. 2. Question-Answering Research Assistant : A research assistant that retrieves and summarizes content from academic papers, legal contracts, or corporate reports in response to user queries. This assistant saves time and enhances accessibility to large, complex documents. 3. Legal Assistants : A legal assistant capable of retrieving case laws, statutes, and regulations to assist lawyers in building arguments. 4. Conversational Agentic RAG Based Medical Assistants : A healthcare chatbot that retrieves symptom information, treatment guidelines, or medication details from medical knowledge bases and references. This agent provides factually accurate and trustworthy medical advice to users. The agent memory allows patients and health providers to track patient history. If the agent uses tools it can provide third party app or built in tools to facilitate analysis and perform some supportive tasks. Also: Improved patient access: Offers immediate access to healthcare information and support, even outside of regular clinic hours. Patient education: Can explain complex medical terms in simple language, promoting better understanding of conditions. Reduced burden on healthcare providers: Can handle routine questions and administrative tasks, freeing up medical professionals for more complex cases. Figure 2. Source : CW Lin How does RAG Framework Operate? The Retrieval-Augmented Generation (RAG) framework combines retrieval-based methods with generative models to enhance the quality of responses in tasks such as question answering, information retrieval, and natural language generation. It operates through three primary stages: Retrieve, Augment, and Generate. 1. Retrieve Stage The objective of this stage is to identify and retrieve the most relevant documents from a large corpus based on the user query as follow: a. Query Encoding: Typically a dense encoder like BERT, RoBERTa, or other transformer-based models used The input query ùëÑ is processed through the query encoder to produce a dense vector representation ùê∏ùëû‚Äã. Example: For the query ‚ÄúWhat are the benefits of RAG frameworks? ‚Äù, the query encoder transforms this text into a high-dimensional vector that captures its semantic meaning. b. Document Retrieval Corpus Indexing: The document corpus is pre-encoded into dense vectors using a document encoder, often the same model architecture as the query encoder. Similarity Search: The dense vector ùê∏ùëû‚Äã is compared against the document vectors ùê∏ùëë‚Äã in the corpus. Common similarity measures include cosine similarity and dot product. Ranking and Selection: Documents are ranked based on their similarity to ùê∏ùëû‚Äã. The top-n most relevant documents ùê∑1,ùê∑2,‚Ä¶,ùê∑n are selected for further processing. Example: If n is 3, the system retrieves the top 3 documents that are most similar to the encoded query vector. 2. Augment Stage Combine the user query and the retrieved documents into a structured prompt that can be effectively processed by the generative model. a. Combining Query and Context: Prompt Template: A predefined template is used to combine the query and the retrieved documents. This template organizes the information in a coherent manner. Example Template: \"Query: [USER QUERY] Context: [DOC 1: ...] [DOC 2: ...] [DOC 3: ...]\" Example with Data: \"Query: What are the benefits of RAG frameworks? Context: [DOC 1: RAG frameworks combine retrieval and generation methods to enhance accuracy...] [DOC 2: By using retrieval, RAG frameworks can provide up-to-date information...] [DOC 3: The generative component of RAG synthesizes information from the retrieved documents...]\" b. Formatting and Special Tokens: Special Tokens: Special tokens or delimiters (e.g., ‚Äú[CTX]‚Äù, ‚Äú[DOC]‚Äù, ‚Äú[SEP]‚Äù) are used to clearly delineate the different parts of the input (query and each document). This helps the model understand the structure and boundaries of the input data as following example: \"Query: What are the benefits of RAG frameworks? [CTX] [DOC 1: RAG frameworks combine retrieval and generation methods to enhance accuracy...] [SEP] [DOC 2: By using retrieval, RAG frameworks can provide up-to-date information...] [SEP] [DOC 3: The generative component of RAG synthesizes information from the retrieved documents...]\" 3. Generate Stage Objective: Produce a coherent, contextually relevant response based on the combined query and retrieved documents. a. Input Preparation: Tokenization: The combined query and context prompt is tokenized into sub-word units or tokens that the generative model can process as bellow example: \"Query: What are the benefits of RAG frameworks? [CTX] [DOC 1: RAG frameworks combine retrieval and generation methods to enhance accuracy...] [SEP] [DOC 2: By using retrieval, RAG frameworks can provide up-to-date information...] [SEP] [DOC 3: The generative component of RAG synthesizes information from the retrieved documents...]\" This would be tokenized into a sequence of tokens like: [‚ÄúQuery‚Äù, ‚Äú:‚Äù, ‚ÄúWhat‚Äù, ‚Äúare‚Äù, ‚Äúthe‚Äù, ‚Äúbenefits‚Äù, ‚Äúof‚Äù, ‚ÄúRAG‚Äù, ‚Äúframeworks‚Äù, ‚Äú?‚Äù, ‚Äú[CTX]‚Äù, ‚ÄúRAG‚Äù, ‚Äúframeworks‚Äù, ‚Äúcombine‚Äù, ‚Äúretrieval‚Äù, ‚Äúand‚Äù, ‚Äúgeneration‚Äù, ...]. b. Contextual Generation Generative Model: Models like GPT-3, BART, or T5 are used for this stage. These models are designed to generate text sequences based on input prompts. Attention Mechanisms: The model uses attention mechanisms to focus on the most relevant parts of the input context while generating each token in the response. Sequential Decoding: The model generates the response token by token, predicting each subsequent token based on the query and the context until the response is complete. Example Response: \"RAG frameworks provide several benefits, including enhanced accuracy by combining retrieval and generation methods, the ability to provide up-to-date information, and the synthesis of comprehensive responses from retrieved documents.\" Press enter or click to view image in full size Figure 3. RAG Workflow, Figure by the Author Comparative Analysis of LLM Optimization Techniques and RAG Retrieval-Augmented Generation (RAG) emerges as a prominent LLM optimization technique, often compared to Fine-tuning (FT) and prompt engineering. These methods exhibit distinct characteristics with respect to their reliance on external knowledge and model adaptation requirements, as illustrated in Figure 4. Press enter or click to view image in full size Figure 4. RAG compared with other model optimization methods in the aspects of ‚ÄúExternal Knowledge Required‚Äù and ‚ÄúModel Adaption Required‚Äù. Source Figure 4 effectively visualizes the differentiation among these three methods across two key dimensions. External Knowledge Requirements reflects the extent to which each method leverages external knowledge sources during the optimization process. Model Adaptation Requirements captures the degree of modification or fine-tuning required within the LLM itself. Prompt engineering represents a minimally invasive approach. It capitalizes on the inherent capabilities of a pre-trained LLM, minimizing the need for both external knowledge and model adaptation. Essentially, prompt engineering optimizes the LLM through refined prompts or instructions, allowing it to leverage its existing knowledge base more effectively. In contrast, RAG functions analogously to equipping the LLM with a specialized, tailored textbook. This external knowledge base (textbook) facilitates precise information retrieval during the generation process.expand_more As a result, RAG is ideally suited for tasks demanding accurate retrieval of specific information. Fine-tuning (FT) , on the other hand, bears a resemblance to a student gradually internalizing knowledge over an extended period. This approach involves fine-tuning the LLM on a specific dataset tailored to the desired task.expand_more This iterative process enables the LLM to replicate specific structures, styles, or formats present within the training data. Consequently, FT is well-suited for scenarios where mimicking a particular style or format is crucial. A Simple Implementation of RAG To illustrate the practical implementation of Retrieval-Augmented Generation (RAG), let‚Äôs consider a code example that demonstrates how to set up, fine-tune, and utilize a RAG system using a document corpus. The RAG process involves three main steps: retrieval, augmentation, and generation. First, we define a document corpus containing various pieces of information about RAG, and we create query-document pairs for training. This helps the model learn the relationship between queries and relevant documents. The SentenceTransformer model is used as the retriever to encode the documents into embeddings, which are then indexed using Faiss, a library for efficient similarity search. The retriever model is fine-tuned using the MultipleNegativesRankingLoss to improve its ability to fetch relevant documents based on queries. The fine-tuning step is crucial as it optimizes the retriever model to better understand and retrieve pertinent documents from the corpus. Once the retriever is fine-tuned, the retrieval step involves encoding a query, searching for the top relevant documents, and creating a context by concatenating these documents. This context is then passed to the generator model, a pre-trained GPT-2 model, which generates a coherent and informative response. The generator model takes the query and the augmented context to produce the final output, leveraging the information provided by the retriever. By adjusting parameters such as max_new_tokens , temperature , and top_p , we can control the length, creativity, and relevance of the generated response, ensuring it is focused and accurate. from sentence_transformers import SentenceTransformer, InputExample, losses, datasets, LoggingHandler from torch.utils.data import DataLoader import logging import faiss import torch # Setup logging logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO, handlers=[LoggingHandler()]) # Define an expanded document corpus document_corpus = [ \"RAG stands for Retrieval-Augmented Generation, a method in natural language processing.\", \"It combines the power of retrieval-based models with generative models to improve response quality.\", \"The retriever fetches relevant documents based on a query.\", \"The generator uses the retrieved documents to generate a coherent and informative response.\", \"This approach leverages large-scale pre-trained models for both retrieval and generation tasks.\", \"Fine-tuning RAG involves training both the retriever and generator components.\", \"RAG can be used in various applications, including chatbots, question answering systems, and more.\", \"The framework was introduced by Facebook AI Research (FAIR) in 2020.\", \"RAG aims to improve the informativeness and accuracy of generated responses.\", \"The retriever component of RAG can be based on various architectures like BM25 or dense retrieval models.\", \"Generative models in RAG are typically based on architectures like BERT, GPT, or T5.\", \"RAG can handle large-scale knowledge bases and provide specific answers to queries.\", \"The retriever in RAG selects relevant passages, which are then used by the generator to produce an answer.\", \"One of the key benefits of RAG is its ability to provide contextually rich and accurate responses.\", \"Training RAG requires a large and diverse dataset to cover a wide range of possible queries.\", \"RAG has shown significant improvements over traditional retrieval-based or generative models alone.\", \"The architecture of RAG allows it to be fine-tuned for specific tasks or domains.\", \"RAG integrates retrieval and generation in a seamless manner, improving overall system performance.\", \"The use of retrieval-augmented generation helps in reducing hallucinations in generated text.\", \"RAG's design allows it to leverage external knowledge sources effectively.\", \"The application of RAG extends to areas like medical diagnosis, legal advice, and customer support.\", \"By using retrieval-augmented techniques, RAG ensures that the responses are grounded in real data.\", \"The flexibility of RAG makes it suitable for various languages and dialects.\", \"RAG's performance can be enhanced by continuously updating the knowledge base with new information.\", \"Researchers are exploring ways to make RAG more efficient and scalable for real-time applications.\", \"The integration of retrieval and generation in RAG provides a powerful tool for AI developers.\", \"RAG's ability to access and utilize large datasets makes it a valuable asset in data-intensive fields.\", \"Future developments in RAG could lead to more advanced and autonomous AI systems.\" ] # Define an expanded set of query-document pairs for training query_document_pairs = [ (\"What does RAG stand for?\", document_corpus[0]), (\"How does RAG improve response quality?\", document_corpus[1]), (\"What is the role of the retriever in RAG?\", document_corpus[2]), (\"How does the generator work in RAG?\", document_corpus[3]), (\"What is the advantage of using RAG?\", document_corpus[4]), (\"How do you fine-tune RAG?\", document_corpus[5]), (\"What are the applications of RAG?\", document_corpus[6]), (\"Who introduced RAG and when?\", document_corpus[7]), (\"What is the goal of RAG?\", document_corpus[8]), (\"What architectures can the retriever in RAG be based on?\", document_corpus[9]), (\"What architectures are used for the generative model in RAG?\", document_corpus[10]), (\"How does RAG handle large-scale knowledge bases?\", document_corpus[11]), (\"What is the process of retrieving and generating answers in RAG?\", document_corpus[12]), (\"What are the key benefits of using RAG?\", document_corpus[13]), (\"What is required to train RAG effectively?\", document_corpus[14]), (\"How does RAG compare to traditional models?\", document_corpus[15]), (\"Can RAG be fine-tuned for specific tasks?\", document_corpus[16]), (\"How does RAG integrate retrieval and generation?\", document_corpus[17]), (\"How does RAG reduce hallucinations in generated text?\", document_corpus[18]), (\"How does RAG leverage external knowledge sources?\", document_corpus[19]), (\"What are the applications of RAG in specialized fields?\", document_corpus[20]), (\"How does RAG ensure responses are grounded in real data?\", document_corpus[21]), (\"Can RAG be used for different languages?\", document_corpus[22]), (\"How can RAG's performance be enhanced over time?\", document_corpus[23]), (\"What are researchers focusing on to improve RAG?\", document_corpus[24]), (\"How does RAG benefit AI developers?\", document_corpus[25]), (\"Why is RAG valuable in data-intensive fields?\", document_corpus[26]), (\"What are future developments expected in RAG?\", document_corpus[27]) ] # Create training examples train_examples = [] for i, (query, document) in enumerate(query_document_pairs): train_example = InputExample(guid=str(i), texts=[query, document]) train_examples.append(train_example) # Define a Sentence Transformer model for the retriever retriever_model = SentenceTransformer('paraphrase-MiniLM-L6-v2') # Create a Faiss index for the retriever corpus_embeddings = retriever_model.encode(document_corpus, convert_to_tensor=True) index = faiss.IndexFlatL2(corpus_embeddings.shape[1]) index.add(corpus_embeddings.cpu().numpy()) # Prepare the data loader for training train_dataset = datasets.SentencesDataset(train_examples, model=retriever_model) train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=2) train_loss = losses.MultipleNegativesRankingLoss(retriever_model) # Train the retriever model retriever_model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=100) # Define a simple tokenizer and generator model from transformers import GPT2LMHeadModel, GPT2Tokenizer generator_model = GPT2LMHeadModel.from_pretrained('gpt2') generator_tokenizer = GPT2Tokenizer.from_pretrained('gpt2') generator_tokenizer.pad_token = generator_tokenizer.eos_token # Set the pad token to be the same as the eos token def generate_response(query): query_embedding = retriever_model.encode(query, convert_to_tensor=True).unsqueeze(0) # Ensure 2D array D, I = index.search(query_embedding.cpu().numpy(), k=3) # Top 3 documents retrieved_docs = [document_corpus[int(idx)] for idx in I[0]] # Fetch indices from the first element of I # Create input for the generator context = \" \".join(set(retrieved_docs)) # Ensure unique sentences context = \" \".join(context.split()[:100]) # Limit context length input_text = f\"query: {query} context: {context}\" input_ids = generator_tokenizer.encode(input_text, return_tensors='pt') # Generate response with adjusted parameters output_ids = generator_model.generate( input_ids, max_new_tokens=50, # Control the length of new tokens generated num_return_sequences=1, temperature=0.7, top_p=0.9, do_sample=True, repetition_penalty=2.0, # Add repetition penalty to reduce repeated phrases pad_token_id=generator_tokenizer.eos_token_id ) response = generator_tokenizer.decode(output_ids[0], skip_special_tokens=True) return response # Test the RAG system query = \"What is RAG?\" response = generate_response(query) print(\"Generated Response:\", response) Output: Generated Response: query: What is RAG? context: RAG can be used in various applications, including chatbots, question answering systems, and more. RAG stands for Retrieval-Augmented Generation, a method in natural language processing. The application of RAG extends to areas like medical diagnosis, legal advice, and customer support. The generated response is focused but still heterogeneous . To further refine the output and ensure relevance, we can: Increase the specificity of context: Use fewer retrieved documents for context or prioritize the most relevant document. Further tune generation parameters: Adjust max_new_tokens, temperature, top_p, and repetition_penalty as needed. Consider using a more advanced or fine-tuned model: If using GPT-2, switching to GPT-3 or a fine-tuned version might yield better results. Post-Processing: Implement post-processing steps to clean up the generated text, removing any remaining irrelevant parts. The link of full code and the process of iteratively improvement of the RAG is Here in my kaggle notebook. Also in my article ‚ÄúCreating LLM-Powered Research Assistant Chatbot with Gemma 2, Langchain, and Faiss‚Äù you can dive into langchain and build a simple research assistant with Gemma 2. RAG Ecosystem Ther are further concepts and components to bundle RAG ecosystem , as depicted in Figure 5. emphasizeing RAG‚Äôs significant advancement in enhancing the capabilities of LLMs by integrating parameterized knowledge from language models with extensive non-parameterized data from external knowledge bases, the evolution of RAG technologies and their application on many different tasks, three developmental paradigms within the RAG framework: Naive, Advanced, and Modular RAG, each representing a progressive enhancement over its predecessors, RAG‚Äôs technical integration with other AI methodologies, such as fine-tuning and reinforcement learning. Despite the progress in RAG technology, there are research opportunities to improve its robustness and its ability to handle extended contexts. RAG‚Äôs application scope is expanding into multimodal domains, adapting its principles to interpret and process diverse data forms like images, videos, and code. This expansion highlights RAG‚Äôs significant practical implications for AI deployment, attracting interest from academic and industrial sectors. Press enter or click to view image in full size Figure 5. RAG Ecosystem, Source References: Yunfan Gao et al. 2024, ‚ÄúRetrieval-Augmented Generation for Large Language Models: A Survey‚Äù Paulo Finardi et al. 2024, ‚ÄúThe Chronicles of RAG: The Retriever, the Chunk and the Generator‚Äù Lewis, Patrick, et al. 2020, ‚ÄúRetrieval-augmented generation for knowledge-intensive nlp tasks.‚Äù Kunal Sawarkar et al. 2024, ‚ÄúBlended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy with Semantic Search and Hybrid Query-Based Retrievers‚Äù Retrieval Augmented Generation Research: 2017‚Äì2024 (30 Paper Summaries) by Moritz Wallawitsch A Practitioners Guide to Retrieval Augmented Generation (Blog post)"
}