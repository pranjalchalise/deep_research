{
  "metadata": {
    "key": "page:https://medium.com/@shuv.sdr/langgraph-architecture-and-design-280c365aaf2c",
    "created": 1769900924.209093,
    "ttl": 604800
  },
  "data": "LangGraph — Architecture and Design Shuvrajyoti Debroy 20 min read · Aug 24, 2025 -- Listen Share Press enter or click to view image in full size Source: Aisera Overview AI agents are categorized based on — Intelligence level Decision-making process Interaction with their environment The six main types, from simplest to most advanced, are — Simple Reflex Agent Model-Based Reflex Agent Goal-Based Agent Utility-Based Agent Learning Agent Multi Agent Systems Simple Reflex Agent — “Reacts” Logic — Follows fixed if-condition → then-action rules. Inputs — Percepts from sensors → immediate action. Example — Thermostat turns on heat if temperature < 18°C. Strengths — Fast and effective in predictable environments. Limitations — No memory of past states Can’t adapt to new or unexpected conditions Model-Based Reflex Agent — “Remembers” Logic — Uses condition-action rules + an internal model of the world. State Component — Stores information on environment changes and effects of its own actions. Example — Robotic vacuum remembers cleaned areas, obstacles, and inferred unseen spaces. Strengths — Tracks past states and predicts changes Handles partially observable environments Limitations — Still reactive, no planning toward long-term goals. Goal-Based Agent — “Aims” Logic — Chooses actions based on desired goals , not just conditions. Process — Uses internal model to predict outcomes of possible actions Selects actions that move closer to goal Example — Self-driving car plans turns to reach destination X. Strengths — Can plan and adapt to achieve goals. Limitations — Any method that reaches the goal is acceptable — no evaluation of quality. Utility-Based Agent — “Evaluates” Logic — Considers how desirable each possible outcome is. Utility Function — Scores outcomes based on preferences (e.g., efficiency, safety, cost). Example — Drone chooses fastest, safest, and most energy-efficient delivery route, not just any route to the address. Strengths — Chooses optimal actions based on ranked preferences Balances trade-offs between goals Limitations — Requires accurate utility function design. Learning Agent — “Improves” Logic — Learns and adapts from experience. Components — Critic — Compares performance to a standard and gives reward/punishment feedback. Learning Element — Updates decision-making rules based on feedback. Problem Generator — Suggests new actions to explore. Performance Element — Executes chosen actions. Example — AI chess bot learns strategies from thousands of games, adjusting after losses. Strengths — Most adaptable Improves over time without manual reprogramming Limitations — Slowest to optimize Data- and compute-intensive Multi-Agent Systems Combine multiple agent types in a shared environment. Agents cooperate toward common goals. Particularly powerful when learning agents integrate generative AI for reasoning and adaptability. Still, most real-world applications benefit from human-in-the-loop oversight. Agent Architectures Reflection Agents Two LLM roles — Generator & Reflector . Iteratively refine outputs by reviewing past responses. Use LangChain for structured prompts and memory. LangGraph manages message flow and state. Reflexion Agents Add self-critique + external tools . Provide structured, cited outputs. Loop until verifiable result is achieved. ReAct Agents Use reasoning steps — Thought → Action → Action Input → Observation → Final Answer . Call tools only when necessary. Maintain message history for contextual reasoning. Multi-Agent Systems Specialized agents collaborate using graph-structured workflows . Common patterns — Pipeline — sequential handoffs. Hub-and-Spoke — central coordinator dispatching tasks. Generalist agents coordinate specialists . Orchestration frameworks — LangGraph, CrewAI, AutoGen, BeeAI Agentic RAG Enhances RAG by letting an LLM act as a decision-maker , not just a responder. Chooses most relevant data source based on query context . Boosts accuracy , adaptability , and cross-industry applicability . LangGraph — Architechture Modern agent architectures enable AI systems to critique and refine their own output for higher quality. These “self-improving” agents use loops where the agent reviews its work and acts on feedback. LangGraph — a graph-based framework for stateful LLM applications — makes it easy to implement these patterns. LangGraph represents agents as graphs of states and nodes. The state (often a message history) flows through nodes (functions or LLM calls) linked by edges with conditional logic. LangGraph is an advanced, low-level framework within the LangChain ecosystem, designed for stateful, multi-agent AI applications . It models agent workflows as graphs , giving developers full control without restrictive abstractions, making it ideal for complex, adaptive AI systems . Implement multi-agent workflows using LangGraph, a graph-based framework for orchestrating AI agents working together to complete complex tasks. LangGraph structures AI workflows as directed graphs where each node represents an agent or processing step, and edges control the flow of data and execution between them. The shared state enables collaboration among all agents. Key Benefits Modular design with independently testable agents Dynamic routing based on runtime conditions Shared memory accessible by all nodes Visual, clear workflow representation At a high level, these can be categorizes as three approaches — Reflection agents , reflexion agents , and ReAct agents . Each uses a different strategy for self-improvement — Reflection agents Prompts the model to review its own answer (like a teacher grading its work). Reflexion agents Adds external feedback (search or tools) to guide corrections. ReAct agents Alternate reasoning and actions, thinking and doing in a loop (tool calls, chain-of-thought). Why Use Graph Architecture Limitations of Traditional Approaches — Linear & Limited — Loops and if-statements can branch or repeat but lack persistent, rich state. LangChain DAGs — Chains are directed acyclic graphs: effective for one-pass, linear tasks but unable to natively support loops, retries, or complex branching. Static Behavior — In a traditional chain, if retrieval fails once, the system is stuck with poor results. Advantages of LangGraph’s Graph Architecture — Explicit State Management — Shared context persists across nodes and is continuously updated. Conditional Transitions — Branching and routing adapt dynamically at runtime. Dynamic Decision-Making — Agents can revisit steps, retry actions, or refine outputs based on evolving conditions. Human-in-the-Loop — Workflows can pause for manual oversight while retaining full context. Modularity — Nodes can be developed, tested, and reused independently. Enhanced Observability — Clear visualization of execution paths (e.g., via Mermaid diagrams) simplifies debugging and monitoring. Adaptive Behavior — Agents can loop back, refine queries, and retry retrieval, enabling continuous improvement rather than one-shot execution. Practical Example — Customer Support Agent Traditional Control — Limited to simple, repetitive checks. A basic while-loop may repeatedly ask for input but quickly forget prior topics, offering little adaptability. LangGraph Workflow — Enables branching conversations, looping retries, and persistent memory of past interactions. It can pause for human intervention and later resume, maintaining full conversational context — resulting in a far more adaptive and reliable support system. When to use LangGraph? LangGraph is ideal for complex agent workflows that need explicit state and flexible control flow. Use it when your task involves — Loops or iteration Tasks where the agent might try an action, check results, and repeat until a goal is achieved. (for example, iterative refinement of a query or planning steps.) Conditional branching Workflows with if/else logic. For instance, a support bot that asks follow-up questions based on user replies. Long-running processes Scenarios where the agent must persist state and resume after delays or failures (LangGraph supports durable execution and checkpointing). Complex state management When many variables or data points must be carried through the workflow, LangGraph’s shared state object is more explicit than passing context through nested chains. Multi-a,gent or multi-step coordination You can design graphs where different nodes represent different agents or tools working together, with the central state tracking their interactions. LangGraph Workflow Structure State — State is the shared, central piece of data that flows through your LangGraph workflow. Think of it as a dictionary (or, more formally, a `TypedDict` or `Pydantic` model) that carries all relevant information from one node to the next. Each node in the graph reads from and updates this state object. Use TypedDict to structure state. State may include lists, nested dictionaries, message sequences, etc. from typing import TypedDict class WorkflowState(TypedDict): user_query: str summary: str step_count: int Initialize a state field with an initial value (e.g., {\"user_query\": \"Hello\", \"summary\": \"\", \"step_count\": 0} ) when invoking the graph. Document processing scenario — Validate uploaded document. Extract text. Analyze content. Generate summary. from typing import TypedDict class DocumentProcessingState(TypedDict): file_path: str text_content: str summary: str analysis_results: dict StateGraph — StateGraph is the controller or blueprint of the workflow. It is a class provided by LangGraph that lets you define — - What nodes exist - How they connect (edges) - Where the workflow starts and ends - When to loop or branch conditionally In other words, State is the data that flows through the system (changes during execution) but a StateGraph is the structure that defines how that data moves and gets transformed (fixed once compiled). You create a StateGraph by passing the state schema type — from langgraph.graph import StateGraph graph = StateGraph(WorkflowState) Nodes— Each node is a Python function (or LangChain Runnable) that takes the state dict and returns an updated state. Nodes perform actions such as calling an LLM, running a tool, computing something, etc. For example: def summarize(state: WorkflowState) -> WorkflowState: text = state[\"user_query\"] state[\"summary\"] = llm_summarize(text) # some LLM call return state You can add nodes to the graph using graph.add_node() . Each node should update the state and return it. LangGraph can also use LangChain chains or agents as nodes (they must conform to the same state signature). Transform or observe state. Use state unpacking to update immutably. Edges — Edges define how the workflow moves from one node to the next. Linear (normal) edges — Use graph.add_edge(from_node, to_node) to always flow from one node to the next. You must specify an entry point and exit using the special START and END tokens from langgraph.graph. from langgraph.graph import START, END graph.add_edge(START, \"summarize\") graph.add_edge(\"summarize\", \"finalize\") graph.add_edge(\"finalize\", END) Here, we add the edges from START to summarize , indicating that the graph workflow will begin from summarize . After that, we have two other edges, one from summarize to finalize and another from finalize to END indicating the end of the workflow Conditional edges — Use graph.add_conditional_edges(from_node, decision_func, mapping) to branch. The decision_func(state) should return a string key; then, the workflow moves to whichever node name that key maps to. def decide(state: WorkflowState) -> str: return \"repeat\" if state[\"step_count\"] < 2 else \"done\" graph.add_conditional_edges( \"summarize\", decide, {\"repeat\": \"summarize\", \"done\": END} ) In this case, after “summarize” is executed, decide() function checks step_count . If it returns \"repeat\", the graph loops back to the \"summarize\" node again; if \"done\", it goes to the special END and stops. Conditional edges let LLM-driven or logic-driven functions choose the next step dynamically. Define execution flow. Conditional edges branch based on state. Execution (Compile and Run) — Once all nodes and edges are added, call `runnable = graph.compile()`. This produces a Runnable object (just like a LangChain Runnable) that you can run with `.invoke(initial_state)` or `.stream(initial_state)`. runnable = graph.compile() final_state = runnable.invoke({\"user_query\": \"Hello\", \"summary\": \"\", \"step_count\": 0}) This executes the graph: it starts at START , follows edges (running each node's function on the state), and stops at END . The final state dict contains all updates. The compiled graph supports all usual LangChain methods (.stream(), async variants, batching, etc.) Use StateGraph to compile and run with invoke() . Visualizing with Mermaid diagram — LangGraph supports generating Mermaid diagrams, a lightweight syntax for rendering flowcharts and state diagrams. This helps you visually understand how your agent moves from one node to another, especially when there are loops or conditional branches. print(app.get_graph().draw_mermaid()) State Management — Before implementing, you need to define a shared state type that all agents will read from and update. from typing import TypedDict, List, Optional class SalesReportState(TypedDict): request: str raw_data: Optional[dict] processed_data: Optional[dict] chart_config: Optional[dict] report: Optional[str] errors: List[str] next_action: str Agent Nodes — Agents are functions that receive the shared state, perform their task, and return the updated state. Below are simplified placeholders to illustrate this pattern. Agent Function Placeholders — def data_collector_agent(state: SalesReportState) -> SalesReportState: # Placeholder: collect raw data based on request # Update state with raw_data and set next_action return state def data_processor_agent(state: SalesReportState) -> SalesReportState: # Placeholder: process raw_data and update processed_data # Set next_action to next step return state def chart_generator_agent(state: SalesReportState) -> SalesReportState: # Placeholder: create chart configuration from processed_data # Update chart_config and set next_action return state def report_generator_agent(state: SalesReportState) -> SalesReportState: # Placeholder: generate textual report using processed_data # Update report and set next_action to complete return state def error_handler_agent(state: SalesReportState) -> SalesReportState: # Placeholder: handle errors, prepare error messages in report # Set next_action to complete return state Routing Logic — The workflow requires a router to decide which agent to run next based on the current state. def route_next_step(state: SalesReportState) -> str: routing = { \"collect\": \"data_collector\", \"process\": \"data_processor\", \"visualize\": \"chart_generator\", \"report\": \"report_generator\", \"error\": \"error_handler\", \"complete\": \"END\" } return routing.get(state.get(\"next_action\", \"collect\"), \"END\") Building and Compiling the Workflow Graph — Using LangGraph’s StateGraph , you add nodes for each agent, define conditional edges based on the routing function, and set the entry point. from langgraph.graph import StateGraph, END def create_sales_report_workflow(): workflow = StateGraph(SalesReportState) workflow.add_node(\"data_collector\", data_collector_agent) workflow.add_node(\"data_processor\", data_processor_agent) workflow.add_node(\"chart_generator\", chart_generator_agent) workflow.add_node(\"report_generator\", report_generator_agent) workflow.add_node(\"error_handler\", error_handler_agent) workflow.add_conditional_edges(\"data_collector\", route_next_step, { \"data_processor\": \"data_processor\", \"error_handler\": \"error_handler\", END: END }) workflow.add_conditional_edges(\"data_processor\", route_next_step, { \"chart_generator\": \"chart_generator\", \"error_handler\": \"error_handler\", END: END }) workflow.add_conditional_edges(\"chart_generator\", route_next_step, { \"report_generator\": \"report_generator\", \"error_handler\": \"error_handler\", END: END }) workflow.add_conditional_edges(\"report_generator\", route_next_step, { \"error_handler\": \"error_handler\", END: END }) workflow.add_conditional_edges(\"error_handler\", route_next_step, {END: END}) workflow.set_entry_point(\"data_collector\") return workflow.compile() Running the Workflow — Once compiled, the workflow can be invoked with an initial state. This runs the agents in order, respecting the routing logic. def run_sales_report_workflow(): app = create_sales_report_workflow() initial_state = SalesReportState( request=\"Q1-Q2 2024 Sales Analysis\", raw_data=None, processed_data=None, chart_config=None, report=None, errors=[], next_action=\"collect\" ) print(\"Starting workflow...\\n\") final_state = app.invoke(initial_state) print(\"\\nWorkflow Complete\\n\") if final_state[\"errors\"]: print(\"Errors:\") for err in final_state[\"errors\"]: print(f\"- {err}\") print(\"\\nFinal Report:\\n\", final_state[\"report\"]) return final_state if __name__ == \"__main__\": run_sales_report_workflow() Visualization — Multi-Agent Workflow Graph Workflows can be visualized as Mermaid diagrams , making it easier to see nodes, edges, and branching logic . This improves debugging and understanding of complex AI workflows. This diagram illustrates the multi-agent workflow constructed using LangGraph for the sales report generation system. Press enter or click to view image in full size Nodes — Each rectangle represents an individual agent responsible for a specific task — data_collector — Gathers the raw sales data based on the user request. data_processor — Analyzes and processes the collected data. chart_generator — Prepares chart configurations for data visualization. report_generator — Produces the final textual sales report. error_handler — Manages errors and generates error reports if any step fails. END — Represents the termination of the workflow. Edges — Directed arrows show the flow of control between agents based on routing logic — From each agent, the workflow can proceed to the next logical agent if successful. If an error occurs, the workflow routes to the error_handler agent for recovery. The workflow terminates by reaching the END node. Dynamic Routing — The routing decisions are based on the current state’s next_action value, enabling flexible and conditional progression through the agents. Error Handling — Multiple agents can trigger the error handler, which centralizes error management and ensures graceful workflow completion. This visualization captures the modular and scalable nature of LangGraph multi-agent workflows, highlighting how individual agents collaborate through shared state and conditional transitions to accomplish complex tasks reliably. This example demonstrated a simple multi-agent system using LangGraph with — Separate agents handling data collection, processing, visualization, and reporting. Dynamic routing based on agent outcomes. Shared state passed and updated by each agent. A clean, maintainable workflow that can be extended with error handling or parallelism. A LangGraph Example In this example, let’s build an increment counter using LangGraph. Define the state schema — We start with defining the State Schema with a `TypedDict` (or `Pydantic` model) listing all fields your workflow needs. Example: from typing import TypedDict class GraphState(TypedDict): count: int message: str This says our state has an integer count and a string message . Initialize the StateGraph — from langgraph.graph import StateGraph graph = StateGraph(GraphState) Add nodes — For each step, write a function that takes and returns the state. Then register it with `add_node()`. Example: def increment(state: GraphState) -> GraphState: state[\"count\"] += 1 state[\"message\"] = f\"Count is now {state['count']}\" return state graph.add_node(\"increment\", increment) You can add as many nodes as needed, possibly using the same function multiple times under different names. Connect edges — Define the flow of execution. At minimum, set a start edge from `START`, and usually end at `END`. For linear flow: from langgraph.graph import START, END graph.add_edge(START, \"increment\") graph.add_edge(\"increment\", END) Conditional branching (optional) — To loop or branch, use `add_conditional_edge()`. For example, to repeat the “increment” node until the count reaches 3: def decide_next(state: GraphState) -> str: return \"again\" if state[\"count\"] < 3 else \"finish\" graph.add_conditional_edges(\"increment\", decide_next, {\"again\": \"increment\", \"finish\": END}) Now, after each “increment”, the graph checks the returned key: if “again”, it loops back to “increment” (making a cycle); if “finish”, it goes to END . This simple loop will run the increment node three times. Compile and invoke — Finally, compile the graph and run it — app = graph.compile() result = app.invoke({\"count\": 0, \"message\": \"\"}) Here, the initial state has count=0. After invoking, the result contains the updated state (e.g., count = 3 if we looped three times). State Design Best Practices State holds the workflow’s context and shared data. Key design principles include — Clear Naming — Use descriptive names like user_query or agent_response . Flat Structures — Avoid deeply nested states for easier manipulation. Example — from typing import TypedDict class SupportAgentState(TypedDict): user_input: str agent_response: str issue_type: str retry_count: int Node Design Principles Each node should perform a single, clear task — processing , validation , integration and decision Processing Nodes — Perform data transformation or computation. Validation Nodes — Check conditions or data integrity. Integration Nodes — Interface with external systems (APIs, databases). Decision Nodes — Direct workflow paths based on conditions. Nodes communicate through state — Read necessary inputs from state. Perform the task. Update the state accordingly. Edge and Workflow Patterns Edges control execution flow between nodes. Common patterns include — Simple Conditional Logic — def route_decision(state): if state[\"retry_count\"] > 2: return \"human_review\" elif state[\"issue_type\"] == \"resolved\": return \"end_interaction\" else: return \"continue_processing\" Error Handling Strategies Always plan for errors — Include error-specific state fields. Create dedicated error-handling nodes. Implement graceful fallbacks. Common strategies — Retry Nodes — Attempt an action again. Error Nodes — Route to human intervention or logging systems after repeated failures. Testing and Debugging Maintain testable and debuggable workflows — Node Isolation — Test nodes individually. Predictable States — Same inputs should produce the same outputs. Incremental Development — Add and verify nodes step-by-step. Performance Considerations Ensure efficient workflow execution — Minimize state complexity. Isolate costly computations in specific nodes. Use caching for repeated expensive operations. Integration Tips Connecting external systems — Separate integration logic clearly. Anticipate failures with timeouts and fallback mechanisms. Incorporating humans effectively — Pause workflows for approvals or reviews clearly. Provide straightforward paths for human decisions. Common Mistakes to Avoid Avoid — Oversized nodes handling multiple tasks. Deeply nested or unclear states. Ignoring error conditions. Instead — Use modular nodes with clear responsibilities. Explicitly define state schemas. Plan clearly for error handling early in design. LangGraph — Reflection agents Reflection agents use internal critique to refine outputs. Conceptually, the agent first generates an initial answer, then a second step reflects on that answer. The reflector (often role-played as a teacher or critic) points out flaws or suggests improvements. The agent may loop this generate-then-reflect cycle a few times to polish the answer. Workflow User > — Generator > — Reflector > — User Press enter or click to view image in full size Mechanics Typically, one node calls the LLM to produce a response, and another node calls the LLM to critique or improve it. A simple LangGraph MessageGraph can model this two-step loop. Below, generate_answer and critique_answer are two nodes. We loop between them until a max step count is reached. Pseudo Code from langgraph.graph import MessageGraph, END from langchain_core.messages import HumanMessage, AIMessage # Node that generates an initial response def generate_answer(state): # (In practice, call an LLM here) answer = \"This is my first attempt.\" return {\"messages\": state[\"messages\"] + [AIMessage(content=answer)]} # Node that critiques and refines the previous answer def critique_answer(state): # (In practice, call LLM to critique) critique = \"The answer is incomplete; add more detail.\" return {\"messages\": state[\"messages\"] + [AIMessage(content=critique)]} builder = MessageGraph() builder.add_node(\"generate\", generate_answer) builder.add_node(\"reflect\", critique_answer) builder.set_entry_point(\"generate\") # Loop control: alternate until max iterations MAX_STEPS = 3 def should_continue(state): return \"reflect\" if len(state[\"messages\"]) < 2*MAX_STEPS else END builder.add_conditional_edges(\"generate\", should_continue) builder.add_edge(\"reflect\", \"generate\") graph = builder.compile() # Run the reflection agent initial_message = HumanMessage(content=\"Explain photosynthesis.\") result = graph.invoke({\"messages\": [initial_message]}) print(result[\"messages\"][-1]) # Final answer or critique This makes the agent self-critique its answer. In practice, the reflector node is prompted to evaluate the generator’s output and return suggestions. The loop continues until no more revisions are needed or a limit is reached. When to use Reflection is useful for creative or open-ended tasks (e.g., drafting text, answering complex questions) where iterative refinement helps. It adds overhead (extra LLM calls) but often yields clearer, more thorough answers. However, since it only relies on the model’s own reasoning (no outside data), the final answer may not improve much unless the reflector catches errors. Use Reflection when you want basic iterative self-improvement without adding external searches or tools. LangGraph — Reflexion agents Reflexion agents formalize the idea of reflection with external grounding. Here the agent not only critiques its output, but also uses external information or citations to do so. Each cycle typically involves three steps — Draft (initial response) The agent generates an answer and may propose search queries (or tool calls) to gather facts. Execute tools These queries are run (for example, web search) and results are added to the context. Revise A “revisor” node has the agent analyze the draft answer plus fetched info, and explicitly list missing or incorrect parts. Reflexion forces the agent to cite sources and enumerate what’s missing, making corrections more effective. In LangGraph, we chain three nodes in a loop (Draft → Execute Tools → Revise) until no further revisions are needed or a maximum iteration. Workflow User > — Responder > — Execute Tools > — Revisor > — User Press enter or click to view image in full size Mechanics Each iteration adds more grounding. For example, after the draft answer, the agent might search Wikipedia, then the revise step reads the search results and updates the answer. The revised answer goes back into the loop if needed. Pseudo Code from langgraph.graph import MessageGraph, END from langchain_core.messages import HumanMessage, AIMessage, SystemMessage def draft_answer(state): # (LLM draft; could also generate search query) response = \"The capital of France is Paris.\" return {\"messages\": state[\"messages\"] + [AIMessage(content=response)]} def execute_tools(state): # (Simulate external info; e.g., search results) info = \"París (France) - capital: Paris (en.wikipedia.org)\" return {\"messages\": state[\"messages\"] + [SystemMessage(content=info)]} def revise_answer(state): # (LLM re-evaluates answer using info) revision = \"Yes, France's capital is Paris. I've verified this.\" return {\"messages\": state[\"messages\"] + [AIMessage(content=revision)]} # Build graph builder = MessageGraph() builder.add_node(\"draft\", draft_answer) builder.add_node(\"execute_tools\", execute_tools) builder.add_node(\"revise\", revise_answer) builder.add_edge(\"draft\", \"execute_tools\") builder.add_edge(\"execute_tools\", \"revise\") # Loop control: stop after N iterations MAX_LOOPS = 2 def continue_reflexion(state): # Count assistant messages to determine iteration iteration = sum(1 for m in state[\"messages\"] if isinstance(m, AIMessage)) return \"execute_tools\" if iteration <= MAX_LOOPS else END builder.add_conditional_edges(\"revise\", continue_reflexion) builder.set_entry_point(\"draft\") graph = builder.compile() initial_message = HumanMessage(content=\"What is the capital of France?\") result = graph.invoke({\"messages\": [initial_message]}) # Final revised answe This agent uses a built-in search or tool execute_tools to ground its critique. The revise node then updates the answer explicitly (e.g., adding evidence or fixing errors). The process stops when the agent judges the answer is good or after a set number of loops When to use Reflexion is ideal when accuracy or factual grounding matters. Because it enforces evidence (citations) and points out missing info, it shines on fact-checking, research, or coding tasks where correctness is critical. It is more complex and slower (requires search/tool calls), but yields highly vetted answers. Use Reflexion Agents for tasks like data lookup, code generation with static analysis, or any QA requiring references. LangGraph — ReAct Agents ReAct (Reason + Act) agents interleave thinking and action. Rather than a separate “reflector” step, a ReAct agent alternates between internal reasoning (chain-of-thought) and taking actions (tool calls, function calls) in one workflow. Each cycle, the agent decides what to do, does it, then reasons again on the updated state. Workflow User > — Agent > — Thought > — Action > — Observation > — Answer The architecture is often conditional on whether more tools are needed. Press enter or click to view image in full size Mechanics The agent first uses the LLM to reason or plan (e.g., “I will search for the capital”). This might result in either a final answer or a tool request. If a tool call is needed, the agent calls it (e.g., a search API), adds the observation, and then thinks again with the new info. This continues until the agent outputs a final answer. Below is a simplified version for a weather agent (no actual API calls) showcasing ReAct (pseudocode). We define a StateGraph where the state includes a message history and logic flow — Pseudo Code from langgraph.graph import StateGraph, END from langchain_core.messages import HumanMessage, AIMessage # Simple state with messages and a step counter def call_model(state): # (LLM reasons; may request an action or give an answer) last = state[\"messages\"][-1] if \"weather\" in last: # chain-of-thought leading to an action thought = AIMessage(content=\"Let me find the weather for you.\") return {\"messages\": state[\"messages\"] + [thought]} else: # final answer answer = AIMessage(content=\"It's sunny in NYC today.\") return {\"messages\": state[\"messages\"] + [answer]} def call_tool(state): # (Simulate a weather API/tool result) tool_result = AIMessage(content=\"Weather(temperature=75F, condition=sunny)\") return {\"messages\": state[\"messages\"] + [tool_result]} # Decide whether to act or finish based on last message def next_step(state): last = state[\"messages\"][-1] if \"find the weather\" in last: return \"tools\" return \"end\" # Build the graph graph = StateGraph(dict) # using a plain dict state graph.add_node(\"think\", call_model) graph.add_node(\"act\", call_tool) graph.set_entry_point(\"think\") # If the model's message triggers an action, go to 'act'; else end. graph.add_conditional_edges(\"think\", next_step, {\"tools\": \"act\", \"end\": END}) graph.add_edge(\"act\", \"think\") compiled = graph.compile() result = compiled.invoke({\"messages\": [HumanMessage(content=\"What is the weather in NYC?\")]}) print(result[\"messages\"][-1]) # Final assistant answer ``` Here the agent thinks (calls the model) and acts (calls a tool) alternately. The next_step function checks the content of the last assistant message to decide. In practice, a ReAct agent's prompt would instruct the model to output either an action or the final answer, and LangGraph routes accordingly. When to use ReAct is best for tasks that require tool use or complex planning, like interacting with APIs, databases, or multi-step reasoning. Because it weaves in actions dynamically, it can adapt to tasks (e.g., “Call calculator tool then interpret output”). It is simpler than Reflexion but more powerful than a basic chain-of-thought. Use ReAct agents when you need the model to reason and perform external actions in sequence. For quick setups, LangGraph even offers create_react_agent to instantiate a standard ReAct pattern with one call. Comparison of Agent Styles Press enter or click to view image in full size Conclusion Effective LangGraph architecture focuses on simplicity, clarity, and modularity — Begin simply and incrementally add complexity. Keep state structures explicit and manageable. Design independent, clearly defined nodes. Proactively handle potential errors. Adopting these principles will help you create robust and maintainable LangGraph workflows tailored to your specific AI needs. Key Takeaways LangGraph is built for sophisticated AI agents requiring — Dynamic decision-making Long-term context Modularity & debugging tools Its graph-based approach with nodes, edges, and persistent state enables more flexible, maintainable, and powerful AI workflows than traditional programming structures. Summary LangGraph is an advanced framework designed for building stateful, multiagent applications. Nodes are functions that do the actual computation. Edges define how the execution flows from one step to the next. State is a shared memory that remembers everything across nodes. LangGraph’s unique capabilities include looping and branching for making dynamic decisions, state persistence to maintain context over long interactions, human-in-the-loop functionality for timely human interventions, and time travel to facilitate convenient debugging. LangGraph offers state management, allowing the workflow to maintain and modify context across different nodes. It also offers conditional transitions, enabling the workflow to make decisions at runtime and branch accordingly. A LangGraph workflow can branch, loop, pause for human input, and resume execution, all while preserving full conversational memory. LangGraph graphs can be visualized using Mermaid diagrams with core primitives such as nodes and edges clearly represented. LangChain helps developers build LLM-powered applications using modular components like prompts, memory, and tools. LangGraph, on the other hand, extends LangChain’s capabilities by enabling stateful, multiagent workflows State in LangGraph is a complex, evolving memory that contains all inputs, intermediate values, and outputs. Nodes are functions that process the current state. Some nodes modify the state, whereas others are used for side effects. Edges define how the execution flows between nodes, passing the updated state from one step to the next. Conditional edges allow the workflow to make dynamic decisions, routing the state to different nodes. Building a LangGraph application involves creating a StateGraph object, incorporating nodes, connecting them, setting an entry point, and then compiling the graph into a runnable application. Running a LangGraph workflow is done by invoking the compiled application with an initial state. Workflow visualization helps to understand the execution flow and how the state progresses through different nodes."
}