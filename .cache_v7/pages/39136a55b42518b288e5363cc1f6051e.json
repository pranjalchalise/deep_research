{
  "metadata": {
    "key": "page:https://www.researchgate.net/scientific-contributions/Lee-Spector-2197894265",
    "created": 1769899405.68249,
    "ttl": 604800
  },
  "data": "What is this page? This page lists works of an author who doesn't have a ResearchGate profile or hasn't added the works to their profile yet. It is automatically generated from public (personal) data to further our legitimate goal of comprehensive and accurate scientific recordkeeping. If you are this author and want this page removed, please let us know . Publications (39) Dominated Novelty Search: Rethinking Local Competition in Quality-Diversity Conference Paper July 2025 · 8 Reads · 3 Citations Ryan Bahlous-Boldi · Maxence Faldor · Luca Grillotti · [...] · Antoine Cully +2 Dominated Novelty Search: Rethinking Local Competition in Quality-Diversity Preprint File available February 2025 · 48 Reads · 1 Citation Ryan Bahlous-Boldi · Maxence Faldor · Luca Grillotti · [...] · Antoine Cully Quality-Diversity is a family of evolutionary algorithms that generate diverse, high-performing solutions through local competition principles inspired by natural evolution. While research has focused on improving specific aspects of Quality-Diversity algorithms, surprisingly little attention has been paid to investigating alternative formulations of local competition itself -- the core mechanism distinguishing Quality-Diversity from traditional evolutionary algorithms. Most approaches implement local competition through explicit collection mechanisms like fixed grids or unstructured archives, imposing artificial constraints that require predefined bounds or hard-to-tune parameters. We show that Quality-Diversity methods can be reformulated as Genetic Algorithms where local competition occurs through fitness transformations rather than explicit collection mechanisms. Building on this insight, we introduce Dominated Novelty Search, a Quality-Diversity algorithm that implements local competition through dynamic fitness transformations, eliminating the need for predefined bounds or parameters. Our experiments show that Dominated Novelty Search significantly outperforms existing approaches across standard Quality-Diversity benchmarks, while maintaining its advantage in challenging scenarios like high-dimensional and unsupervised spaces. Read more Download +5 Informed Down-Sampled Lexicase Selection: Identifying Productive Training Cases for Efficient Problem Solving Article Publisher preview available December 2024 · 36 Reads · 27 Citations Evolutionary Computation Ryan Bahlous-Boldi · Martin Briesch · Dominik Sobania · [...] · Lee Spector Genetic Programming (GP) often uses large training sets and requires all individuals to be evaluated on all training cases during selection. Random down-sampled lexicase selection evaluates individuals on only a random subset of the training cases, allowing for more individuals to be explored with the same number of program executions. However, sampling randomly can exclude important cases from the down-sample for a number of generations, while cases that measure the same behavior (synonymous cases) may be overused. In this work, we introduce Informed Down-Sampled Lexicase Selection. This method leverages population statistics to build down-samples that contain more distinct and therefore informative training cases. Through an empirical investigation across two different GP systems (PushGP and Grammar-Guided GP), we find that informed down-sampling significantly outperforms random down-sampling on a set of contemporary program synthesis benchmark problems. Through an analysis of the created down-samples, we find that important training cases are included in the down-sample consistently across independent evolutionary runs and systems. We hypothesize that this improvement can be attributed to the ability of Informed Down-Sampled Lexicase Selection to maintain more specialist individuals over the course of evolution, while still benefiting from reduced per-evaluation costs. Read more View access options Generational Computation Reduction in Informal Counterexample-Driven Genetic Programming Preprint File available August 2024 · 2 Reads Thomas Helmuth · Edward Pantridge · James Gunder Frazier · Lee Spector Counterexample-driven genetic programming (CDGP) uses specifications provided as formal constraints to generate the training cases used to evaluate evolving programs. It has also been extended to combine formal constraints and user-provided training data to solve symbolic regression problems. Here we show how the ideas underlying CDGP can also be applied using only user-provided training data, without formal specifications. We demonstrate the application of this method, called ``informal CDGP,'' to software synthesis problems. Our results show that informal CDGP finds solutions faster (i.e. with fewer program executions) than standard GP. Additionally, we propose two new variants to informal CDGP, and find that one produces significantly more successful runs on about half of the tested problems. Finally, we study whether the addition of counterexample training cases to the training set is useful by comparing informal CDGP to using a static subsample of the training set, and find that the addition of counterexamples significantly improves performance. Read more Download Effective Adaptive Mutation Rates for Program Synthesis Conference Paper July 2024 · 6 Reads · 1 Citation Andrew Ni · Lee Spector Solving Deceptive Problems Without Explicit Diversity Maintenance Conference Paper July 2024 · 8 Reads · 5 Citations Ryan Bahlous-Boldi · Li Ding · Lee Spector Facilitating Function Application in Code Building Genetic Programming Conference Paper July 2024 · 4 Reads · 1 Citation Thomas Helmuth · Jayden Fedoroff · Edward Pantridge · Lee Spector Improving Lexicase Selection with Informed Down-Sampling Conference Paper July 2024 · 8 Reads Martin Briesch · Ryan Bahlous-Boldi · Dominik Sobania · [...] · Lee Spector A Comprehensive Analysis of Down-sampling for Genetic Programming-based Program Synthesis Conference Paper July 2024 · 10 Reads · 1 Citation Ryan Bahlous-Boldi · Ashley Bao · Martin Briesch · [...] · Alexander Michael Lalejini Effective Adaptive Mutation Rates for Program Synthesis Preprint June 2024 · 7 Reads Andrew Ni · Lee Spector The problem-solving performance of many evolutionary algorithms, including genetic programming systems used for program synthesis, depends on the values of hyperparameters including mutation rates. The mutation method used to produce some of the best results to date on software synthesis benchmark problems, Uniform Mutation by Addition and Deletion (UMAD), adds new genes into a genome at a predetermined rate and then deletes genes at a rate that balances the addition rate, producing no size change on average. While UMAD with a predetermined addition rate outperforms many other mutation and crossover schemes, we do not expect a single rate to be optimal across all problems or all generations within one run of an evolutionary system. However, many current adaptive mutation schemes such as self-adaptive mutation rates suffer from pathologies like the vanishing mutation rate problem, in which the mutation rate quickly decays to zero. We propose an adaptive bandit-based scheme that addresses this problem and essentially removes the need to specify a mutation rate. Although the proposed scheme itself introduces hyperparameters, we either set these to good values or ensemble them in a reasonable range. Results on software synthesis and symbolic regression problems validate the effectiveness of our approach. Read more Citations (18) ... These methods search over expressive families of attentionbased, diffusion-inspired, or transformer-style operators [19]- [21]. Specifically, they seek to automatically discover evolutionary strategies by learning from a task distribution, such as LQD [22] and the Evolution Transformer [23]. ... Reference: Learning to Evolve with Convergence Guarantee via Neural Unrolling Dominated Novelty Search: Rethinking Local Competition in Quality-Diversity Citing Conference Paper July 2025 Ryan Bahlous-Boldi · Maxence Faldor · Luca Grillotti · [...] · Antoine Cully ... Selection based on individual fitness instances has also been explored, particularly through Lexicase Selection [19,42], where parents are chosen based on their performance on specific instances (or objectives) rather than an overall aggregated fitness measure. This approach has been utilized across various types of evolutionary computation, including symbolic regression in GP [26] and multi-objective optimization [39], having shown good results in both performance and diversity maintenance [5] . ... Reference: On the Dynamics of Mating Preferences in Genetic Programming Solving Deceptive Problems Without Explicit Diversity Maintenance Citing Conference Paper July 2024 Ryan Bahlous-Boldi · Li Ding · Lee Spector ... To evaluate and select individuals during evolution, we employ a down-sampled selection strategy, where a subset of test cases is sampled without replacement from the training set at each generation [95][96] [97] . Specifically, although the dataset contains 1000 input-output pairs, only 50 training cases are randomly selected at the start of each generation and used for fitness evaluation. This approach ensures diverse assessments across generations, preventing overfitting to a fixed subset of test cases while maintaining time efficiency. ... Reference: Exploring the Effect of Genetic Improvement for Large Language Models-Generated Code A Comprehensive Analysis of Down-sampling for Genetic Programming-based Program Synthesis Citing Conference Paper July 2024 Ryan Bahlous-Boldi · Ashley Bao · Martin Briesch · [...] · Alexander Michael Lalejini ... The authors manage to circumvent the vanishing mutation rate problem, which happens when the evolved mutation rates converge to zero. Similar ideas, in particular adaptive bandit-based schemes, have been applied to program synthesis problems [19] . ... Reference: Exploring Higher-Order Mutation Rates in a Game-Theoretical Setting Effective Adaptive Mutation Rates for Program Synthesis Citing Conference Paper July 2024 Andrew Ni · Lee Spector ... In the worst case, lexicase will evaluate all individuals on all test cases in each generation, resulting in prohibitive run-times for large populations ( ) or high numbers of fitness cases ( ): ( × ). At the same time, there is no obvious way to take advantage of parallelism or single-instruction-multipledata (SIMD) architectures for speed-up [34] . ... Reference: A comparison of tournament and lexicase selection paradigms in regression problems: error-based fitness versus correlation fitness DALex: Lexicase-Like Selection via Diverse Aggregation Citing Conference Paper March 2024 Lecture Notes in Computer Science Andrew Ni · Lee Spector · Li Ding ... Unfortunately, subsets of training cases generated by RDS often contain many similar cases that bring in only little new information, while it potentially excludes other, more rare edge or corner cases which are often much more informative for GP. Ignoring such rare but relevant edge cases for several generations potentially harms the overall performance of RDS [3] . ... Reference: ROIDS: Robust Outlier-Aware Informed Down-Sampling Informed Down-Sampled Lexicase Selection: Identifying Productive Training Cases for Efficient Problem Solving Citing Article Publisher preview available December 2024 Evolutionary Computation Ryan Bahlous-Boldi · Martin Briesch · Dominik Sobania · [...] · Lee Spector ... If cases are solved by different candidates, these cases more likely test distinct behaviors and therefore contribute to a more diverse subset of training cases. The literature confirms these design decisions, as recent work finds that IDS outperforms RDS in the domain of program synthesis [3, 4] and has been studied in combination with different selection methods [1,2]. ... Reference: ROIDS: Robust Outlier-Aware Informed Down-Sampling A Static Analysis of Informed Down-Samples Citing Conference Paper July 2023 Ryan Bahlous-Boldi · Alexander Lalejini · Thomas Helmuth · Lee Spector ... If cases are solved by different candidates, these cases more likely test distinct behaviors and therefore contribute to a more diverse subset of training cases. The literature confirms these design decisions, as recent work finds that IDS outperforms RDS in the domain of program synthesis [3,4] and has been studied in combination with different selection methods [1, 2]. ... Reference: ROIDS: Robust Outlier-Aware Informed Down-Sampling The Problem Solving Benefits of Down-sampling Vary by Selection Scheme Citing Conference Paper July 2023 Ryan Bahlous-Boldi · Ashley Bao · Martin Briesch · [...] · Alexander Lalejini ... This principle is in the essence of Lexicase selection [15,36], a parent selection method where each individual is selected based on their local performance at each test case (randomly ordered), therefore following a specialist approach rather than an overall fitness measure. Lexicase selection has been successfully applied to several problems such as Neural Network Optimization [4] and Evolutionary Robotics [39], having also shown promising results in GP applied to symbolic regression [5], making it a state-of-the-art parent selection method [9] . ... Reference: Desire-Driven Selection: An Epigenetic Experiment in Genetic Programming Probabilistic Lexicase Selection Citing Conference Paper July 2023 Li Ding · Edward Pantridge · Lee Spector ... Tree depth, specifying the maximum depth of GP individuals, ranged from 3 to 5, balancing complexity and computational efficiency. Lastly, we used tournament selection [59], which is currently one of the most widely used methods for selection [60] , with a tournament size of 4. ... Reference: Sensing Through Tissues Using Diffuse Optical Imaging and Genetic Programming Analyzing the Interaction Between Down-Sampling and Selection Citing Preprint April 2023 Ryan Bahlous-Boldi · Ashley Bao · Martin Briesch · [...] · Alexander Michael Lalejini"
}