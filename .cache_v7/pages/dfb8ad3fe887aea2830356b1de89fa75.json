{
  "metadata": {
    "key": "page:https://medium.com/data-science-in-your-pocket/andrew-ngs-agentic-reviewer-ai-for-research-paper-reviews-1c2d9cda8086",
    "created": 1769905610.518212,
    "ttl": 604800
  },
  "data": "Andrew NG’s Agentic Reviewer : AI for Research Paper Reviews Get Research Paper reviewed in minutes using AI Mehul Gupta 6 min read · Dec 4, 2025 -- 1 Listen Share Press enter or click to view image in full size Photo by UX Indonesia on Unsplash I still remember the first paper I helped submit as a junior researcher. We waited. And waited. And waited some more. Six months later, three anonymous paragraphs landed in the inbox. One reviewer was annoyed about missing baselines we did include, another wanted more experiments but didn’t say which ones mattered, and the third basically summarized our own abstract back at us. Not feedback. Judgment vibes. We fixed whatever we could guess and re-submitted into the same black hole. Generative AI books Visit Amazon's Generative AI books Page and shop for all Generative AI books books. Check out pictures, author… www.amazon.in Andrew Ng’s “Agentic Reviewer” exists because that whole loop is broken. The origin story is almost painfully normal: a student outside Stanford had a paper rejected six times over three years. Six review cycles. Roughly half a year between each round. Most of the feedback was vague, noisy, and tilted toward deciding acceptance instead of helping the author improve. That’s the exact opposite of what early-stage research needs. So Ng and Yixing Jiang started tinkering with what they call an “agentic reviewer,” initially as a weekend project. The goal was simple: compress the feedback cycle from months to minutes. No promise of replacing human reviewers. No grand slogans about democratizing science. Just basic tooling to let a researcher iterate faster instead of waiting endlessly for delayed, low-signal commentary. What This “Agentic Reviewer” Actually Does At the surface level, the tool looks simple. You upload your paper PDF. Optionally choose a target venue. You get back a structured review with actionable feedback. Maybe a score if the venue is ICLR. Underneath, the pipeline is quietly smart in places where older LLM review demos usually fall flat. Step 1: Document Understanding The first agent converts your PDF into Markdown using LandingAI’s Agentic Document Extraction system. This matters more than it sounds. Most LLM-based reviewing setups fail at the “can you even read the paper consistently” stage: equations get scrambled, sections drop out, tables melt into nonsense. Markdown normalization stabilizes the input so later stages aren’t hallucinating off bad parses. The agent then extracts the title and sanity-checks whether the document is even an academic paper. If it looks like garbage or non-research, the pipeline stops early. No wasted tokens reviewing a random slide deck disguised as a submission. Step 2: Grounding via arXiv Search This is the core differentiator. Instead of asking the model to review your paper “from memory,” the agent actively searches arXiv for related work. And not just one naive keyword query. It generates multiple search phrases at different specificity levels: Nearby benchmarks and baselines Papers solving the same problem Methodologically related papers Those queries run through the Tavily search API, pulling metadata from arXiv: titles, authors, abstracts. From there, the agent scores relevance and selects the highest-signal related work. This is subtle but important: context length is finite, so the agent can’t dump 50 PDFs into the prompt. It triages first. Then, for top papers, it chooses either: Use the existing arXiv abstract if that’s sufficient Or download the full PDF, convert it to Markdown, and generate a focused summary aligned to salient technical aspects Now the review isn’t generic “LLM vibes” anymore . It is grounded in what’s actually been published recently rather than what the base model half-remembers from training data. Step 3: Structured Review Generation Finally, the agent synthesizes: The Markdown of your paper The summaries of related work And generates a full review using a consistent template. This is where feedback becomes useful rather than decorative. Instead of shallow praise and nitpicks, the output aims for things like: Are claims supported by experiments? Which baselines are missing or outdated based on very recent arXiv work? Where novelty seems weak relative to adjacent papers? Where clarity breaks down technically? Basically the kind of review you wish you’d actually gotten after a real peer cycle, instead of vague “insufficient comparisons” drive-by comments. From Reviewer Chatbot to Human-Level Calibration The boldest claim around this project is not that the reviews read “good.” That’s subjective garbage. The team did something better: quantitative evaluation against real conference data. They trained the scoring head using public ICLR 2025 reviews . Here’s how it worked: Instead of telling the LLM to output a single acceptance score directly (which tends to be noisy and overconfident), the agent scores each paper across seven dimensions : Originality Importance of the research question Claim support Experimental soundness Clarity of writing Value to the research community Contextualization relative to prior work These dimension scores are fed into a linear regression model trained on 150 ICLR submissions that have human reviewer ratings. The remaining 147 papers are used for evaluation. Why regression instead of prompt-hacking a single number? Because calibration matters. Humans don’t decide accept/reject off a gut scalar value; they subconsciously weight multiple axes. This setup mimics that structure more faithfully. And the results were borderline uncomfortable. Spearman correlation: Between two human reviewers: 0.41 Between AI and one human reviewer: 0.42 Which means the agent agrees with any single human reviewer at least as often as humans agree with each other . Acceptance prediction metrics were weaker, but understandably so: Human AUC: 0.84 AI AUC: 0.75 Not apples-to-apples, since acceptance decisions were themselves driven partly by those same human reviews. In other words: the agent is not perfect, but it’s playing in the same statistical noise band as actual peer reviewers. Which is not flattering to humans, but also not fake marketing. Limitations They Don’t Pretend Away To their credit, the team doesn’t oversell this thing. Because grounding relies heavily on arXiv, the tool works best in: AI ML Computer vision NLP Fields with paywalled literature or slower publication pipelines aren’t as well supported yet. It also only handles English-language papers right now. And the reviews can still contain errors since everything is AI-generated. This is an assistant, not a judgment authority. They explicitly discourage using the tool to generate conference reviews that violate submission ethics. In other words: it’s positioned as a feedback accelerator , not a peer-review replacement. Which honestly is the correct framing. Why This Matters More Than the Scores The real win is not the 0.42 correlation headline. That’s just noise dressing. What matters is this change: Instead of: Submit → Wait 6 months → Get noisy feedback → Guess what to fix → Resubmit → Repeat You now get: Submit → Review in minutes → Run experiments or rewrite → Submit again the same day if needed This fundamentally alters research workflow. It shifts iteration cycles closer to how engineering already works: short loops, rapid testing, correction without bureaucratic latency. Anyone who’s done serious ML work knows how much progress stalls waiting for external signals. This tool doesn’t eliminate reviewers. It prevents wasted months before you even know if your direction is viable. Related Work and the Bigger Picture There’s growing work around using agents to analyze peer review itself, generate collaborative reviewer discussions, and measure overlap between AI and human feedback. Liang et al. showed GPT-4 feedback overlaps significantly with humans but misses novelty critique more often. Other evaluations also found LLMs over-index on technical validity while neglecting true originality. More recent ICLR studies even explored using LLM feedback to improve human review quality itself, acting almost as a critique-of-critique layer. And beyond review tools, this connects to a much wider push: AI agents generating hypotheses. Running entire experimental loops. Automated end-to-end discovery systems. In that ecosystem, agentic reviewing becomes a scoring function inside a larger self-driving research machine. The Honest Take I don’t see this as a threat to human reviewers. I see it as mercy for authors. Researchers don’t need gatekeepers earlier in the pipeline; they need guides . The current system confuses the two roles constantly. Judgment arrives too early and feedback too late. This tool flips that dynamic: fast feedback first, judgment later. The fact that it statistically behaves like a human reviewer is funny, but not the real story. The real story is time. Six months versus ten minutes. Most academic frustration has nothing to do with quality or fairness and everything to do with velocity. This is velocity. Even if the agent is imperfect, it moves people forward instead of trapping them in calendar purgatory. That alone justifies its existence. And the cleanest part: It’s free. No gated “AI peer review platform,” no enterprise nonsense. Just upload and get signal. For once, a tool that solves an actual research pain instead of creating new workflow busywork. Stanford Agentic Reviewer Get detailed AI feedback on your research paper (free!) paperreview.ai"
}