{
  "metadata": {
    "key": "page:https://docs.langchain.com/oss/python/releases/langchain-v1",
    "created": 1770002860.222116,
    "ttl": 604800
  },
  "data": "Skip to main content Docs by LangChain home page LangChain + LangGraph Search... ⌘ K Search... Navigation Releases What's new in LangChain v1 LangChain LangGraph Deep Agents Integrations Learn Reference Contribute Python Overview Reference LangChain SDK LangGraph SDK Integrations Deep Agents Errors Reference Releases Versioning Changelog Releases LangChain v1 LangGraph v1 Migration guides Policies Release policy Security On this page create_agent Middleware Prebuilt middleware Custom middleware Built on LangGraph Structured output Standard content blocks Benefits Simplified package Namespace langchain-classic Migration guide Reporting issues Additional resources See also LangChain v1 is a focused, production-ready foundation for building agents. We’ve streamlined the framework around three core improvements: create_agent The new standard for building agents in LangChain, replacing langgraph.prebuilt.create_react_agent . Standard content blocks A new content_blocks property that provides unified access to modern LLM features across providers. Simplified namespace The langchain namespace has been streamlined to focus on essential building blocks for agents, with legacy functionality moved to langchain-classic . To upgrade, pip uv Copy pip install -U langchain For a complete list of changes, see the migration guide . ​ create_agent create_agent is the standard way to build agents in LangChain 1.0. It provides a simpler interface than langgraph.prebuilt.create_react_agent while offering greater customization potential by using middleware . Copy from langchain.agents import create_agent agent = create_agent( model = \"claude-sonnet-4-5-20250929\" , tools = [search_web, analyze_data, send_email], system_prompt = \"You are a helpful research assistant.\" ) result = agent.invoke({ \"messages\" : [ { \"role\" : \"user\" , \"content\" : \"Research AI safety trends\" } ] }) Under the hood, create_agent is built on the basic agent loop — calling a model, letting it choose tools to execute, and then finishing when it calls no more tools: For more information, see Agents . ​ Middleware Middleware is the defining feature of create_agent . It offers a highly customizable entry-point, raising the ceiling for what you can build. Great agents require context engineering : getting the right information to the model at the right time. Middleware helps you control dynamic prompts, conversation summarization, selective tool access, state management, and guardrails through a composable abstraction. ​ Prebuilt middleware LangChain provides a few prebuilt middlewares for common patterns, including: PIIMiddleware : Redact sensitive information before sending to the model SummarizationMiddleware : Condense conversation history when it gets too long HumanInTheLoopMiddleware : Require approval for sensitive tool calls Copy from langchain.agents import create_agent from langchain.agents.middleware import ( PIIMiddleware, SummarizationMiddleware, HumanInTheLoopMiddleware ) agent = create_agent( model = \"claude-sonnet-4-5-20250929\" , tools = [read_email, send_email], middleware = [ PIIMiddleware( \"email\" , strategy = \"redact\" , apply_to_input = True ), PIIMiddleware( \"phone_number\" , detector = ( r \" (?: \\+ ? \\d {1,3} [ \\s .- ] ? ) ? \" r \" (?: \\( ? \\d {2,4} \\) ? [ \\s .- ] ? ) ? \" r \" \\d {3,4} [ \\s .- ] ? \\d {4} \" ), strategy = \"block\" ), SummarizationMiddleware( model = \"claude-sonnet-4-5-20250929\" , trigger = { \"tokens\" : 500 } ), HumanInTheLoopMiddleware( interrupt_on = { \"send_email\" : { \"allowed_decisions\" : [ \"approve\" , \"edit\" , \"reject\" ] } } ), ] ) ​ Custom middleware You can also build custom middleware to fit your needs. Middleware exposes hooks at each step in an agent’s execution: Build custom middleware by implementing any of these hooks on a subclass of the AgentMiddleware class: Hook When it runs Use cases before_agent Before calling the agent Load memory, validate input before_model Before each LLM call Update prompts, trim messages wrap_model_call Around each LLM call Intercept and modify requests/responses wrap_tool_call Around each tool call Intercept and modify tool execution after_model After each LLM response Validate output, apply guardrails after_agent After agent completes Save results, cleanup Example custom middleware: Copy from dataclasses import dataclass from typing import Callable from langchain_openai import ChatOpenAI from langchain.agents.middleware import ( AgentMiddleware, ModelRequest ) from langchain.agents.middleware.types import ModelResponse @dataclass class Context : user_expertise: str = \"beginner\" class ExpertiseBasedToolMiddleware ( AgentMiddleware ): def wrap_model_call ( self , request : ModelRequest, handler : Callable[[ModelRequest], ModelResponse] ) -> ModelResponse: user_level = request.runtime.context.user_expertise if user_level == \"expert\" : # More powerful model model = ChatOpenAI( model = \"gpt-5\" ) tools = [advanced_search, data_analysis] else : # Less powerful model model = ChatOpenAI( model = \"gpt-5-nano\" ) tools = [simple_search, basic_calculator] return handler(request.override( model = model, tools = tools)) agent = create_agent( model = \"claude-sonnet-4-5-20250929\" , tools = [ simple_search, advanced_search, basic_calculator, data_analysis ], middleware = [ExpertiseBasedToolMiddleware()], context_schema = Context ) See all 45 lines For more information, see the complete middleware guide . ​ Built on LangGraph Because create_agent is built on LangGraph , you automatically get built in support for long running and reliable agents via: Persistence Conversations automatically persist across sessions with built-in checkpointing Streaming Stream tokens, tool calls, and reasoning traces in real-time Human-in-the-loop Pause agent execution for human approval before sensitive actions Time travel Rewind conversations to any point and explore alternate paths and prompts You don’t need to learn LangGraph to use these features—they work out of the box. ​ Structured output create_agent has improved structured output generation: Main loop integration : Structured output is now generated in the main loop instead of requiring an additional LLM call Structured output strategy : Models can choose between calling tools or using provider-side structured output generation Cost reduction : Eliminates extra expense from additional LLM calls Copy from langchain.agents import create_agent from langchain.agents.structured_output import ToolStrategy from pydantic import BaseModel class Weather ( BaseModel ): temperature: float condition: str def weather_tool ( city : str ) -> str : \"\"\"Get the weather for a city.\"\"\" return f \"it's sunny and 70 degrees in { city } \" agent = create_agent( \"gpt-4.1-mini\" , tools = [weather_tool], response_format = ToolStrategy(Weather) ) result = agent.invoke({ \"messages\" : [{ \"role\" : \"user\" , \"content\" : \"What's the weather in SF?\" }] }) print ( repr (result[ \"structured_response\" ])) # results in `Weather(temperature=70.0, condition='sunny')` Error handling : Control error handling via the handle_errors parameter to ToolStrategy : Parsing errors : Model generates data that doesn’t match desired structure Multiple tool calls : Model generates 2+ tool calls for structured output schemas ​ Standard content blocks Content block support is currently only available for the following integrations: langchain-anthropic langchain-aws langchain-openai langchain-google-genai langchain-ollama Broader support for content blocks will be rolled out gradually across more providers. The new content_blocks property introduces a standard representation for message content that works across providers: Copy from langchain_anthropic import ChatAnthropic model = ChatAnthropic( model = \"claude-sonnet-4-5-20250929\" ) response = model.invoke( \"What's the capital of France?\" ) # Unified access to content blocks for block in response.content_blocks: if block[ \"type\" ] == \"reasoning\" : print ( f \"Model reasoning: { block[ 'reasoning' ] } \" ) elif block[ \"type\" ] == \"text\" : print ( f \"Response: { block[ 'text' ] } \" ) elif block[ \"type\" ] == \"tool_call\" : print ( f \"Tool call: { block[ 'name' ] } ( { block[ 'args' ] } )\" ) ​ Benefits Provider agnostic : Access reasoning traces, citations, built-in tools (web search, code interpreters, etc.), and other features using the same API regardless of provider Type safe : Full type hints for all content block types Backward compatible : Standard content can be loaded lazily , so there are no associated breaking changes For more information, see our guide on content blocks . ​ Simplified package LangChain v1 streamlines the langchain package namespace to focus on essential building blocks for agents. The refined namespace exposes the most useful and relevant functionality: ​ Namespace Module What’s available Notes langchain.agents create_agent , AgentState Core agent creation functionality langchain.messages Message types, content blocks , trim_messages Re-exported from langchain-core langchain.tools @tool , BaseTool , injection helpers Re-exported from langchain-core langchain.chat_models init_chat_model , BaseChatModel Unified model initialization langchain.embeddings Embeddings , init_embeddings Embedding models Most of these are re-exported from langchain-core for convenience, which gives you a focused API surface for building agents. Copy # Agent building from langchain.agents import create_agent # Messages and content from langchain.messages import AIMessage, HumanMessage # Tools from langchain.tools import tool # Model initialization from langchain.chat_models import init_chat_model from langchain.embeddings import init_embeddings ​ langchain-classic Legacy functionality has moved to langchain-classic to keep the core packages lean and focused. What’s in langchain-classic : Legacy chains and chain implementations Retrievers (e.g. MultiQueryRetriever or anything from the previous langchain.retrievers module) The indexing API The hub module (for managing prompts programmatically) langchain-community exports Other deprecated functionality If you use any of this functionality, install langchain-classic : pip uv Copy pip install langchain-classic Then update your imports: Copy from langchain import ... from langchain_classic import ... from langchain.chains import ... from langchain_classic.chains import ... from langchain.retrievers import ... from langchain_classic.retrievers import ... from langchain import hub from langchain_classic import hub ​ Migration guide See our migration guide for help updating your code to LangChain v1. ​ Reporting issues Please report any issues discovered with 1.0 on GitHub using the 'v1' label . ​ Additional resources LangChain 1.0 Read the announcement Middleware guide Deep dive into middleware Agents Documentation Full agent documentation Message Content New content blocks API Migration guide How to migrate to LangChain v1 GitHub Report issues or contribute ​ See also Versioning – Understanding version numbers Release policy – Detailed release policies Edit this page on GitHub or file an issue . Connect these docs to Claude, VSCode, and more via MCP for real-time answers. Was this page helpful? Yes No Changelog Previous What's new in LangGraph v1 Next ⌘ I"
}